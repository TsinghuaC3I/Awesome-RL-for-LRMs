<div align="center">

<img src="figs/survey_logo.png" style="width: 70%;"/>

## A Survey of Reinforcement Learning for Large Reasoning Models

[![Awesome](https://img.shields.io/badge/Awesome-0066CC?style=for-the-badge&logo=awesome-lists&logoColor=white)](https://github.com/sindresorhus/awesome) [![Survey](https://img.shields.io/badge/Paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white)](https://arxiv.org/abs/2509.08827)  [![Github](https://img.shields.io/badge/Awesome--RL--for--LRMs-000000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/TsinghuaC3I/Awesome-RL-Reasoning-Recipes)  [![HF Papers](https://img.shields.io/badge/HF--Paper-%23FFD14D?style=for-the-badge&logo=huggingface&logoColor=black)](https://huggingface.co/papers/2509.08827)

</div>

> We welcome everyone to open an issue for any related work we haven‚Äôt discussed, and we‚Äôll try to address it in the next release!


## üéâ News

- **[2025-09-11]** üî• Excited to release our **RL for LRMs Survey**! We‚Äôll be updating the full list of papers in with a new category structure soon. Check it out: [Paper](https://huggingface.co/papers/2509.08827).
- **[2025-08-15]** üî• Introducing **SSRL**: an investigation for Agentic Search RL without reliance on external search engine. Check it out: [GitHub](https://github.com/TsinghuaC3I/SSRL) and [Paper](https://arxiv.org/abs/2508.10874).
- **[2025-05-27]** üî• Introducing **MARTI**: A Framework for LLM-based Multi-Agent Reinforced Training and Inference. Check it out: [Github](https://github.com/TsinghuaC3I/MARTI).
- **[2025-04-23]** üî• Introducing **TTRL**: an open-source solution for online RL on data without ground-truth labels, especially test data. Check it out: [Github](https://github.com/PRIME-RL/TTRL) and [Paper](https://arxiv.org/abs/2504.16084).
- **[2025-03-20]** üî• We are excited to introduce collection of papers and projects on RL for reasoning models!

## üìñ Contents
- [A Survey of Reinforcement Learning for Large Reasoning Models](#a-survey-of-reinforcement-learning-for-large-reasoning-models)
- [üéâ News](#-news)
- [üìñ Contents](#-contents)
- [üó∫Ô∏è Overview](#Ô∏è-overview)
- [üìÑ Paper List](#-paper-list)
  - [Large Language Models](#large-language-models)
  - [Multimodal Models](#multimodal-models)
  - [Agentic Applications](#agentic-applications)
- [üåü Acknowledgment](#-acknowledgment)
- [üéà Citation](#-citation)


## üó∫Ô∏è Overview

Our survey provides a comprehensive examination of **Reinforcement Learning for Large Reasoning Models**.

<p align="center">
   <img src="figs/teaser.png" alt="Overview of RL for LRMs Survey" style="width: 100%;">
</p>

We organize the survey into five main sections:

1. <u>Foundational Components:</u> Reward design, policy optimization, and sampling strategies
2. <u>Foundational Problems:</u> Key debates and challenges in RL for LRMs
3. <u>Training Resources:</u> Static corpora, dynamic environments, and infrastructure
4. <u>Applications:</u> Real-world implementations across diverse domains
5. <u>Future Directions:</u> Emerging research opportunities and challenges

## üìÑ Paper List

### Large Language Models

| Date      | Project            | Org                                | Intro                                                        | HF Model                                                     | HF Dataset                                                   | Takeaway Messages                                                 |
| --------- | ------------------ | ---------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2025.0102 | PRIME-RL           | THU & UIUC <br /> Shanghai AI Lab      | [Paper](https://arxiv.org/abs/2502.01456)<br />[GitHub](https://github.com/PRIME-RL/PRIME) ![](https://img.shields.io/github/stars/PRIME-RL/PRIME)<br /> [More](#primerl) | [Eurus-2-7B-PRIME](https://huggingface.co/PRIME-RL/Eurus-2-7B-PRIME) <br />[Eurus-2-7B-PRIME-Zero](https://huggingface.co/PRIME-RL/Eurus-2-7B-PRIME-Zero) | [Eurus-2-RL-Data](https://huggingface.co/datasets/PRIME-RL/Eurus-2-RL-Data) | <details><summary>Click</summary>PRIME offers scalable Reinforcement Learning by using dense, token-level implicit rewards derived only from final outcomes. This bypasses costly step-by-step annotations, providing fine-grained feedback to improve sample efficiency and reasoning.</details> |
| 2025.0122 | DeepSeek-R1        | DeepSeek                           | [Paper](https://arxiv.org/abs/2501.12948)<br />[GitHub](https://github.com/deepseek-ai/DeepSeek-R1) ![](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)<br />[More](#deepseek-r1) | [DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1) <br />[DeepSeek-R1-Zero](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero) | ‚Äî‚Äî                                                           | <details><summary>Click</summary>DeepSeek-R1's core contribution is demonstrating large-scale RL from scratch (600B+) without SFT, achieving emergent "aha moments" (self-reflective reasoning) and matching OpenAI o1's performance at 1/30 cost</details> |
| 2025.0122 | Kimi k1.5          | Kimi                               | [Paper](https://arxiv.org/abs/2501.12599)<br />[GitHub](https://github.com/MoonshotAI/Kimi-k1.5) ![](https://img.shields.io/github/stars/MoonshotAI/Kimi-k1.5)<br />[More](#kimi-k1.5) | ‚Äî‚Äî                                                           | ‚Äî‚Äî                                                           | <details><summary>Click</summary>Kimi 1.5 introduces a simplified RL framework that leverages long-context scaling (128k tokens) and improved policy optimization (e.g., online mirror descent) to enhance reasoning and multimodal performance.</details> |
| 2025.0124 | TinyZero           | Berkeley                           | [Twitter](https://x.com/jiayi_pirate/status/1882839370505621655)<br />[GitHub](https://github.com/Jiayi-Pan/TinyZero) ![](https://img.shields.io/github/stars/Jiayi-Pan/TinyZero)<br />[More](#tinyzero) | ‚Äî‚Äî                                                           | [Countdown-Tasks-3to4](https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4) | <details><summary>Click</summary>TinyZero's core contribution is demonstrating that smaller language models (e.g., 1.5B-3B parameters) can develop complex reasoning, search, and self-verification abilities through Reinforcement Learning, replicating capabilities of larger models like DeepSeek R1-Zero at extremely low cost (<$30).</details> |
| 2025.0124 | Open-R1            | Huggingface                        | [GitHub](https://github.com/huggingface/open-r1) ![](https://img.shields.io/github/stars/huggingface/open-r1)        | [OpenR1-Qwen-7B](https://huggingface.co/open-r1/OpenR1-Qwen-7B)<br />[OlympicCoder-7B](https://huggingface.co/open-r1/OlympicCoder-7B)<br />[OlympicCoder-32B](https://huggingface.co/open-r1/OlympicCoder-32B) | [OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k)<br />[codeforces](https://huggingface.co/datasets/open-r1/codeforces) | <details><summary>Click</summary>Open-R1's core contribution is providing the first fully open-source replication and release of the DeepSeek R1-Zero Reinforcement Learning training pipeline. Its main insight or goal is to democratize access to these advanced RL techniques for enhancing LLM reasoning and planning.</details> |
| 2025.0125 | simpleRL-reason    | HKUST                              | [Paper](https://hkust-nlp.notion.site/simplerl-reason)<br />[GitHub](https://github.com/hkust-nlp/simpleRL-reason) ![](https://img.shields.io/github/stars/hkust-nlp/simpleRL-reason)<br />[More](#simplerl) | [Qwen-2.5-Math-7B-SimpleRL-Zero](https://huggingface.co/hkust-nlp/Qwen-2.5-Math-7B-SimpleRL-Zero)<br />[Qwen-2.5-Math-7B-SimpleRL](https://huggingface.co/hkust-nlp/Qwen-2.5-Math-7B-SimpleRL) | [MATH](https://huggingface.co/datasets/EleutherAI/hendrycks_math) | <details><summary>Click</summary>Researchers replicated the DeepSeek-R1-Zero and DeepSeek-R1 training using a 7B model with only 8K MATH examples, achieving strong results on complex mathematical reasoning.</details> |
| 2025.0205 | Demystify-long-cot | CMU                                | [Paper](https://arxiv.org/abs/2502.03373)<br />[GitHub](https://github.com/eddycmu/demystify-long-cot) ![](https://img.shields.io/github/stars/eddycmu/demystify-long-cot)<br />[More](#demystify) | ‚Äî‚Äî                                                           | ‚Äî‚Äî                                                           | <details><summary>Click</summary>The paper elucidates the role of RL in stabilizing and enhancing long CoT reasoning in LLMs, highlighting the necessity of reward shaping and verifiable reward signals for complex reasoning tasks.</details> |
| 2025.0207 | No-aha-moment | Sea AI Lab                                | [Blog](https://oatllm.notion.site/oat-zero)<br />[GitHub](https://github.com/sail-sg/oat-zero) ![](https://img.shields.io/github/stars/sail-sg/oat-zero)<br /> | ‚Äî‚Äî                                                           | [Countdown-Tasks-3to4](https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4)                                                           | <details><summary>Click</summary>This is the first public critique of the 'aha moment' associated with DeepSeek-R1-Zero-style training, suggesting that changes in response length are an intrinsic part of the reinforcement learning dynamics.</details> |
| 2025.0210 | DeepScaler         | Agentica-Org                       | [Blog](https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2)<br />[GitHub](https://github.com/agentica-project/deepscaler) ![](https://img.shields.io/github/stars/agentica-project/deepscaler)<br />[More](#deepscaler) | [DeepScaleR-1.5B-Preview](https://huggingface.co/agentica-org/DeepScaleR-1.5B-Preview) | [DeepScaleR-Preview-Dataset](https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset) | <details><summary>Click</summary>DeepScaleR's core contribution is demonstrating that a small 1.5B parameter model, fine-tuned using scaled Reinforcement Learning (RL) and an iterative context lengthening scheme, can surpass the reasoning performance of larger, state-of-the-art models like OpenAI's O1-Preview on complex benchmarks (e.g., AIME math problems).</details> |
| 2025.0210 | Logic-RL           | MSRA & Ubiquant                    | [Paper](https://arxiv.org/pdf/2502.14768)<br />[GitHub](https://github.com/Unakar/Logic-RL) ![](https://img.shields.io/github/stars/Unakar/Logic-RL)<br />[More](#logicrl) | ‚Äî‚Äî                                                           | [knights-and-knaves](https://huggingface.co/datasets/K-and-K/knights-and-knaves)   [knights-and-knaves-ZH](https://huggingface.co/datasets/Trae1ounG/knights-and-knaves-ZH)  | <details><summary>Click</summary>The paper introduces Logic-RL, a rule-based reinforcement learning framework that enables large language models to develop o3-mini-level reasoning skills through training on logic puzzles. The reasoning capabilities can also be transferred to other domains like math.</details> |
| 2025.0210 | OREAL              | Shanghai AI Lab <br /> SJTU & CUHK | [Paper](https://arxiv.org/abs/2502.06781)<br /> [GitHub](https://github.com/InternLM/OREAL) ![](https://img.shields.io/github/stars/InternLM/OREAL)<br /> [More](#oreal) | [OREAL-32B](https://huggingface.co/internlm/OREAL-32B)  [OREAL-7B](https://huggingface.co/internlm/OREAL-7B)<br />[OREAL-DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/internlm/OREAL-DeepSeek-R1-Distill-Qwen-7B)<br />[OREAL-32B-SFT](https://huggingface.co/internlm/OREAL-32B-SFT)<br />[OREAL-7B-SFT](https://huggingface.co/internlm/OREAL-7B-SFT) | [OREAL-RL-Prompts](https://huggingface.co/datasets/internlm/OREAL-RL-Prompts) | <details><summary>Click</summary>The paper introduces OREAL, a reinforcement learning framework for mathematical reasoning with binary feedback. It proves that behavior cloning on positive samples is sufficient for optimal learning and proposes reward reshaping for negative samples. A token-level reward model addresses sparse rewards in long reasoning chains. OREAL achieves state-of-the-art results on math benchmarks.</details> |
| 2025.0217 | LIMR               | SJTU                               | [Paper](https://arxiv.org/pdf/2502.11886)<br />[GitHub](https://github.com/GAIR-NLP/LIMR) ![](https://img.shields.io/github/stars/GAIR-NLP/LIMR)<br /> [More](#limr) | [LIMR](https://huggingface.co/GAIR/LIMR)                     | [LIMR](https://huggingface.co/datasets/GAIR/LIMR)            | <details><summary>Click</summary>The paper challenges the assumption that scaling up RL training data inherently improves performance in language models, instead finding that a strategically selected subset of 1,389 samples can outperform a full 8,523-sample dataset.</details> |
| 2025.0218 | Open-Reasoner-Zero | StepFun & THU                      | [Paper](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf) <br />[GitHub](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/) ![](https://img.shields.io/github/stars/Open-Reasoner-Zero/Open-Reasoner-Zero)<br />   [More](#openreaon-zero) | [Open-Reasoner-Zero-7B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B)<br />[Open-Reasoner-Zero-32B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B) | [ORZ-Math-57k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data) | <details><summary>Click</summary>The Open-Reasoner-Zero model has achieved notable performance, with Open-Reasoner-Zero-32B outperforming DeepSeek-R1-Zero-Qwen-32B on the GPQA Diamond benchmark while requiring significantly fewer training steps.</details> |
| 2025.0225 | SWE-RL             | FAIR at Meta                       | [Paper](https://arxiv.org/abs/2502.18449)<br />[GitHub](https://github.com/facebookresearch/swe-rl) ![](https://img.shields.io/github/stars/facebookresearch/swe-rl)<br />[More](#swerl) | ‚Äî‚Äî                                                           | ‚Äî‚Äî                                                           | <details><summary>Click</summary>SWE-RL enhances LLMs' code reasoning through RL using open-source software evolution data, achieving state-of-the-art results in software engineering tasks and demonstrating generalized reasoning capabilities beyond coding.</details> |
| 2025.0227 | Med-RLVR              | Microsoft Research | [Paper](https://arxiv.org/pdf/2502.19655)<br />[More](#medrlvr) | ‚Äî‚Äî                                                           | ‚Äî‚Äî                                                           | <details><summary>Click</summary>The Med-RLVR framework demonstrates emergent medical reasoning via RL, achieving performance parity with SFT on in-distribution tasks and improving out-of-distribution generalization, all without explicit reasoning supervision, showcasing RL's potential in medicine.</details> |
| 2025.0303 | VC-PPO             | Bytedance                          | [Paper](https://arxiv.org/abs/2503.01491)<br />[More](#vcppo) | ‚Äî‚Äî                                                           | ‚Äî‚Äî                                                           | <details><summary>Click</summary>VC-PPO (Value-Calibrated PPO) diagnoses PPO's collapse in long CoT tasks as stemming from value function inaccuracies (initialization bias and reward signal decay in long sequences). Its core contribution is modifying PPO with value pretraining and decoupled GAE for actor and critic.</details> |
| 2025.0306 | LCPO-L1            | CMU                                | [Paper](https://arxiv.org/abs/2503.04697)<br />[GitHub](https://github.com/cmu-l3/l1) ![](https://img.shields.io/github/stars/cmu-l3/l1)<br />[More](#lcpol1) | [L1-Qwen-1.5B-Max](https://huggingface.co/l3lab/L1-Qwen-1.5B-Max)<br /> [L1-Qwen-1.5B-Exact](https://huggingface.co/l3lab/L1-Qwen-1.5B-Exact) | ‚Äî‚Äî                                                           | <details><summary>Click</summary>L1 introduces Length Controlled Policy Optimization (LCPO), a RL method enabling precise control over a reasoning model's thinking time (output length) via prompt instructions. It shows that RL effectively controls reasoning duration and unexpectedly enhances even short-chain reasoning capabilities.</details> |
| 2025.0310 | MRT                | CMU                                | [Paper](https://arxiv.org/pdf/2503.07572)<br />[Project](https://cohenqu.github.io/mrt.github.io/)<br />[GitHub](https://github.com/CMU-AIRe/MRT) ![](https://img.shields.io/github/stars/CMU-AIRe/MRT) | ‚Äî‚Äî                                                           | ‚Äî‚Äî                                                           | <details><summary>Click</summary>MRT (Mixed-Reality Trajectory Preferences) introduces a novel method for fine-tuning cooperative LLM agents. It effectively blends human preferences on real interaction trajectories with AI preferences on synthetic variations, improving data efficiency. This mixed-reality approach surpasses purely AI-driven feedback (RLAIF), especially for complex, multi-turn collaborative tasks.</details> |
| 2025.0318 | TOPR               | Mila & Reliant AI                  | [Paper](https://arxiv.org/abs/2503.14286v2)<br />[More](#topr) | ‚Äî‚Äî                                                           | ‚Äî‚Äî                                                           | <details><summary>Click</summary>TOPR (Tapered Off-Policy REINFORCE) introduces a novel RL algorithm for fine-tuning LLMs. Its core contribution is using asymmetric, tapered importance sampling to modify REINFORCE, enabling stable and efficient off-policy learning. This allows reusing past data effectively without the instability often seen in other methods and without needing explicit KL regularization.</details> |
| 2025.0318 | DAPO               | Bytedance <br /> THU               | [Paper](https://arxiv.org/pdf/2503.14476)<br />[GitHub](https://github.com/BytedTsinghua-SIA/DAPO) ![](https://img.shields.io/github/stars/BytedTsinghua-SIA/DAPO)<br />[More](#dapo) | ‚Äî‚Äî                                                           | [DAPO-Math-17k](https://huggingface.co/datasets/BytedTsinghua-SIA/DAPO-Math-17k) | <details><summary>Click</summary>DAPO algorithm introduces four key techniques (Clip-Higher, Dynamic Sampling, Token-Level Loss, Overlong Shaping) for stable and efficient long-chain-of-thought RL training, surpassing previous SoTA results efficiently.</details> |
| 2025.0320 | Open RS          | VNU University of Science & Knovel Engineering Lab                            | [Paper](https://arxiv.org/pdf/2503.16219)<br />[GitHub](https://github.com/knoveleng/open-rs) ![](https://img.shields.io/github/stars/knoveleng/open-rs)<br />[More](#open-rs) | [Open-RS1](https://huggingface.co/knoveleng/Open-RS1)<br />[Open-RS2](knoveleng/Open-RS2)<br />[Open-RS3](https://huggingface.co/knoveleng/Open-RS3) | [open-s1](https://huggingface.co/datasets/knoveleng/open-s1)<br />[open-deepscaler](https://huggingface.co/datasets/knoveleng/open-deepscaler)<br />[open-rs](https://huggingface.co/datasets/knoveleng/open-rs) | <details><summary>Click</summary>The study investigates the potential of RL to improve reasoning in small LLMs. The results demonstrate rapid reasoning gains, with accuracy improvements on mathematical reasoning benchmarks, and highlight the efficacy of RL-based fine-tuning for small LLMs as a cost-effective alternative to large-scale approaches, using high-quality training data.</details> |
| 2025.0321 | Dr. GRPO           | Sea AI Lab                            | [Paper](https://arxiv.org/abs/2503.20783)<br />[GitHub](https://github.com/sail-sg/understand-r1-zero) ![](https://img.shields.io/github/stars/sail-sg/understand-r1-zero)<br />[More](#oat-zero) | [Qwen2.5-Math-7B-Oat-Zero](https://huggingface.co/sail/Qwen2.5-Math-7B-Oat-Zero)<br />[Qwen2.5-Math-1.5B-Oat-Zero](https://huggingface.co/sail/Qwen2.5-Math-1.5B-Oat-Zero)<br />[Llama-3.2-3B-Oat-Zero](https://huggingface.co/sail/Llama-3.2-3B-Oat-Zero) | [MATH](https://huggingface.co/datasets/EleutherAI/hendrycks_math) | <details><summary>Click</summary>This work critically analyzes R1-Zero-like RL training. It reveals base model properties and GRPO algorithm biases (e.g., length bias) significantly impact outcomes. It contributes the efficient, unbiased Dr. GRPO algorithm and an open-source recipe/codebase for better understanding and reproduction.</details> |
| 2025.0321 | FastCuRL           | Tencent Hunyuan                    | [Paper](https://arxiv.org/abs/2503.17287)<br />[GitHub](https://github.com/nick7nlp/FastCuRL) ![](https://img.shields.io/github/stars/nick7nlp/FastCuRL) | [FastCuRL-1.5B-Preview](https://huggingface.co/Nickyang/FastCuRL-1.5B-Preview) | [FastCuRL](https://huggingface.co/datasets/Nickyang/FastCuRL) | <details><summary>Click</summary>FastCuRL introduces a simple, efficient Curriculum RL method for LLMs. Its core contribution uses target perplexity to dynamically scale the standard RL loss (like PPO), creating an effective curriculum without complex reward models or auxiliary components, enabling faster, more stable training.</details> |
| 2025.0328 | ARGO           | Meta                    | [Paper](https://arxiv.org/abs/2503.19612)<br /> | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper derived the Any-Generation Reward Optimization (AGRO) frim the consistency condition across any possible generation of the model. AGRO achieves a better convergence than KL-regularized policy gradient method.</details> |
| 2025.0401 | Z1           | THU                    | [Paper](https://arxiv.org/abs/2504.00810)<br />[GitHub](https://github.com/efficientscaling/Z1) ![](https://img.shields.io/github/stars/efficientscaling/Z1) | [Z1-7B](https://huggingface.co/efficientscaling/Z1-7B) | [Z1-Code-Reasoning-107K](https://huggingface.co/datasets/efficientscaling/Z1-Code-Reasoning-107K) | <details><summary>Click</summary>This paper proposes training LLMs on code-related reasoning trajectories using a curated dataset and a "Shifted Thinking Window" technique. This allows models to reduce excessive thinking tokens, achieving efficient test-time scaling and generalizing reasoning abilities.</details> |
| 2025.0401 | VAPO           | ByteDance Seed                    | [Paper](https://arxiv.org/pdf/2504.05118)<br /> | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>VAPO offers an integrated solution that effectively alleviates value model bias, the presence of heterogeneous sequence lengths, and the sparsity of reward signal.</details> |
|2025.0407 |  ConciseRL  |   Wand AI   | [Paper](https://arxiv.org/pdf/2504.05185) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This work challenges the idea that longer reasoning chains in LLMs inherently mean better accuracy. It uses mathematical analysis of RL principles, particularly PPO, to show that lengthier responses often arise from the optimization process itself, not necessarily improved reasoning.</details> |
| 2025.0409 | AdaRFT           | USC LIME Lab                    | [Paper](https://arxiv.org/abs/2504.05520)<br />[GitHub](https://github.com/uscnlp-lime/verl) ![](https://img.shields.io/github/stars/uscnlp-lime/verl) | ‚Äî‚Äî | [DeepScaleR_Difficulty](https://huggingface.co/datasets/lime-nlp/DeepScaleR_Difficulty) | <details><summary>Click</summary>AdaRFT proposes Adaptive Curriculum Reinforcement Finetuning to improve LLM reasoning training efficiency. It dynamically adjusts task difficulty based on recent reward signals, accelerating learning by keeping challenges optimally balanced. Experiments on competition math benchmarks show up to 2x fewer steps and improved accuracy, using standard PPO with minimal changes.</details> |
| 2025.0410 | Seed-Thinking-v1.5 | ByteDance Seed                         | [GitHub](https://github.com/ByteDance-Seed/Seed-Thinking-v1.5)![](https://img.shields.io/github/stars/ByteDance-Seed/Seed-Thinking-v1.5) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Seed-Thinking-v1.5 is a high-performing reasoning model that combines curated chain-of-thought data, stable reinforcement learning, and advanced infrastructure to achieve strong results across math, coding, and logic tasks.</details> |
| 2025.0410 | d1 & diffu-GRPO | UCLA & Meta| [Paper](https://arxiv.org/pdf/2504.12216)<br />[GitHub](https://github.com/dllm-reasoning/d1) ![](https://img.shields.io/github/stars/dllm-reasoning/d1)<br />[Project](https://dllm-reasoning.github.io)  | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary> This paper propose d1 to adapt pre-trained masked dLLMs into reasoning via a combination of SFT and RL. The RL method used is named diffu-GRPO. </details> |
| 2025.0413 | Skywork-OR1 | Skywork AI| [Paper](https://arxiv.org/abs/2505.22312) <br />[Blog](https://capricious-hydrogen-41c.notion.site/Skywork-Open-Reasoner-Series-1d0bc9ae823a80459b46c149e4f51680)<br />[GitHub](https://github.com/SkyworkAI/Skywork-OR1) ![](https://img.shields.io/github/stars/SkyworkAI/Skywork-OR1)  | [Skywork-OR1-32B-Preview](https://huggingface.co/Skywork/Skywork-OR1-32B-Preview)<br />[Skywork-OR1-7B-Preview](https://huggingface.co/Skywork/Skywork-OR1-7B-Preview)<br />[Skywork-OR1-Math-7B](https://huggingface.co/Skywork/Skywork-OR1-Math-7B) | [Skywork-OR1-RL-Data](https://huggingface.co/datasets/Skywork/Skywork-OR1-RL-Data) | <details><summary>Click</summary> Skywork-OR1 is a series of robust open-source models trained on carefully curated math and code data. The training process incorporates several modifications to the original GRPO, including offline and online data filtering, multi-stage training, and adaptive entropy control. </details> |
| 2025.0415 | DeepMath | Tencent & SJTU| [Paper](https://arxiv.org/pdf/2504.11456)<br />[GitHub](https://github.com/zwhe99/DeepMath) ![](https://img.shields.io/github/stars/zwhe99/DeepMath)  | [zwhe99/DeepMath-Zero-7B](https://huggingface.co/zwhe99/DeepMath-Zero-7B)<br />[zwhe99/DeepMath-Zero-Math-7B](https://huggingface.co/zwhe99/DeepMath-Zero-Math-7B)<br />[zwhe99/DeepMath-1.5B](https://huggingface.co/zwhe99/DeepMath-1.5B)<br />[zwhe99/DeepMath-Omn-1.5B](https://huggingface.co/zwhe99/DeepMath-Omn-1.5B) | [zwhe99/DeepMath-103K](https://huggingface.co/datasets/zwhe99/DeepMath-103K) | <details><summary>Click</summary> DeepMath-103K is a large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. Trained on DeepMath-103K, DeepMath series models achieve state-of-the-art performance on many math benchmarks. </details> |
| 2025.0421 |  LUFFY  |  Shanghai AI Lab  | [Paper](https://arxiv.org/abs/2504.14945)<br />[GitHub](https://github.com/ElliottYan/LUFFY) ![](https://img.shields.io/github/stars/ElliottYan/LUFFY) | [LUFFY-Qwen-Math-7B-Zero](https://huggingface.co/Elliott/LUFFY-Qwen-Math-7B-Zero) </br> [LUFFY-Qwen-Math-1.5B-Zero](https://huggingface.co/Elliott/LUFFY-Qwen-Math-1.5B-Zero) | [Openr1-Math-46k-8192](https://huggingface.co/datasets/Elliott/Openr1-Math-46k-8192) | <details><summary>Click</summary>This paper introduces LUFFY (Learning to reason Under oFF-policY guidance), a framework that augments zero-RL with off-policy reasoning traces. LUFFY dynamically balances imitation and exploration by combining off-policy demonstrations with on-policy rollouts during training.</details> |
| 2025.0423 | TTRL | THU&Shanghai AI Lab | [Paper](https://arxiv.org/abs/2504.16084)<br />[GitHub](https://github.com/PRIME-RL/TTRL) ![](https://img.shields.io/github/stars/PRIME-RL/TTRL) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs).</details> |
| 2025.0430 | Phi-4-reasoning | Mircosoft | [Paper](https://arxiv.org/abs/2504.21318) | [Phi-4-reasoning](https://huggingface.co/microsoft/Phi-4-reasoning) | ‚Äî‚Äî | <details><summary>Click</summary>This paper introduces Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks.</details> |
| 2025.0511 |   BLEUBERI  |     Maryland     | [Paper](https://arxiv.org/abs/2505.11080)<br />[GitHub](https://github.com/lilakk/BLEUBERI) ![](https://img.shields.io/github/stars/lilakk/BLEUBERI) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Demonstrates that BLEU, a simple string-matching metric, can effectively serve as a reward function for instruction-following tasks, rivaling complex reward models.</details> |
| 2025.0512 | INTELLECT-2 |  PrimeIntellect-ai | [Paper](https://arxiv.org/abs/2505.07291)<br />[GitHub](https://github.com/PrimeIntellect-ai/prime-rl) ![](https://img.shields.io/github/stars/PrimeIntellect-ai/prime-rl) | [INTELLECT-2](https://huggingface.co/PrimeIntellect/INTELLECT-2) | ‚Äî‚Äî | <details><summary>Click</summary>INTELLECT-2 is a 32 billion parameter language model trained through a reinforcement learning run leveraging globally distributed, permissionless GPU resources contributed by the community.</details> |
| 2025.0514 |   Qwen3  |   Alibaba Qwen   | [Paper](https://arxiv.org/abs/2505.09388) <br />[GitHub](https://github.com/QwenLM/Qwen3) ![](https://img.shields.io/github/stars/QwenLM/Qwen3) | [Qwen3](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f) | ‚Äî‚Äî| <details><summary>Click</summary>insights and contributions about RL for reasoning within 30 words.</details> |
| 2025.0516 |   Subnetwork RL  |   UIUC   | [Paper](http://arxiv.org/abs/2505.11711) | ‚Äî‚Äî | ‚Äî‚Äî| <details><summary>Click</summary>insights and contributions about RL for reasoning within 30 words.</details> |
| 2025.0516 |   Data Synthesis RL  |   PKU&MIT   | [Paper](http://arxiv.org/abs/2505.17063) </br> [GitHub](https://github.com/gydpku/Data_Synthesis_RL/) ![](https://img.shields.io/github/stars/gydpku/Data_Synthesis_RL) | ‚Äî‚Äî | ‚Äî‚Äî| <details><summary>Click</summary>insights and contributions about RL for reasoning within 30 words.</details> |
| 2025.0519 |  AR-Lopti   |   CUHK   | [Paper](http://arxiv.org/abs/2505.12929)<br />[GitHub](https://github.com/zhyang2226/AR-Lopti) ![](https://img.shields.io/github/stars/zhyang2226/AR-Lopti) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper identifies a critical yet underexplored issue in RL training: low-probability tokens disproportionately influence model updates due to their large gradient magnitudes.</details> |
| 2025.0519 |  AnytimeReasoner   |   Sea AI Lab   | [Paper](https://arxiv.org/pdf/2505.13438)<br />[GitHub](https://github.com/sail-sg/AnytimeReasoner) ![](https://img.shields.io/github/stars/sail-sg/AnytimeReasoner) | ‚Äî‚Äî | [DeepScaleR-Preview-Dataset](https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset) | <details><summary>Click</summary>This paper proposes a framework for optimizing anytime reasoning under arbitrary token budgets, featuring decoupled optimization of thinking and summarization, dense verifiable rewards, and budget relative policy optimization.</details> |
| 2025.0521 |  EM-PT   |   UIUC   | [Paper](http://arxiv.org/abs/2505.15134)<br />[GitHub](https://github.com/shivamag125/EM_PT) ![](https://img.shields.io/github/stars/shivamag125/EM_PT) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper shows that this simple objective alone, without any labeled data, can substantially improve large language models‚Äô (LLMs) performance on challenging math, physics, and coding tasks.</details> |
| 2025.0521 |  NOVER   |   KCL&SJTU   | [Paper](https://arxiv.org/abs/2505.16022)<br />[GitHub](https://github.com/thinkwee/NOVER) ![](https://img.shields.io/github/stars/thinkwee/NOVER) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper presents a verifier-free R1-zero-like training, which enables it to train on any data (beyond math and coding)!</details> |
| 2025.0522 |   AceReason-Nemontron  |   Nvidia  | [Paper](http://arxiv.org/abs/2505.16400) | [AceReason-Nemotron-14B](https://huggingface.co/nvidia/AceReason-Nemotron-14B) | ‚Äî‚Äî| <details><summary>Click</summary>This paper demonstrates  that large-scale RL can significantly enhance the reasoning capabilities of strong, small- and mid-sized models,  achieving results that surpass those of state-of-the-art distillation-based models.</details> |
| 2025.0522 |   KTAE  |   CAS  | [Paper](https://arxiv.org/abs/2505.16826) <br /> [GitHub](https://github.com/xiaolizh1/KTAE) ![](https://img.shields.io/github/stars/xiaolizh1/KTAE) | [KTAE-7B/1.5B](https://huggingface.co/collections/SunW7777/ktae-models-683ffcbb967d022306413892) | ‚Äî‚Äî| <details><summary>Click</summary>This paper improves the calculation method of advantage based on GRPO, providing more fine-grained token-level advantage, and effectively reducing the generation length.</details> |
| 2025.0523 |   QwenLong-L1  |   Qwen-Doc  | [Paper](http://arxiv.org/abs/2505.17667) <br /> [GitHub](https://github.com/Tongyi-Zhiwen/QwenLong-L1) ![](https://img.shields.io/github/stars/Tongyi-Zhiwen/QwenLong-L1) | [QwenLong-L1-32B](https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1-32B) | ‚Äî‚Äî | <details><summary>Click</summary>insights and contributions about RL for reasoning within 30 words.</details> |
| 2025.0523 | Trinity-RFT  |  Alibaba Group   | [Paper](http://arxiv.org/abs/2505.17826)<br />[GitHub](https://github.com/modelscope/Trinity-RFT) ![](https://img.shields.io/github/stars/modelscope/Trinity-RFT) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Trinity-RFT is a general-purpose, flexible and scalable framework designed for reinforcement fine-tuning (RFT) of large language models.</details> |
| 2025.0524 | LlamaRL | Meta | [Paper](https://arxiv.org/abs/2505.24034) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Distributed async RL framework for LLMs, achieving 10√ó training speed over DeepSpeed; scales to 405B parameters.</details> |
| 2025.0525 |  SeRL  |  ZJU  | [Paper](http://arxiv.org/abs/2505.20347)<br />[GitHub](https://github.com/wantbook-book/SeRL) ![](https://img.shields.io/github/stars/wantbook-book/SeRL) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper proposes Self-play Reinforcement Learning (SeRL) to bootstrap LLM training with limited initial data.</details> |
| 2025.0525 |  BRIDGE   |   CMU  | [Paper](https://arxiv.org/abs/2505.18917)<br />[GitHub](https://github.com/czp16/Bridge-LLM-reasoning)  ![](https://img.shields.io/github/stars/czp16/Bridge-LLM-reasoning) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>The paper proposes behavior injection, a task-agnostic data augmentation method that enhances the effectiveness of reinforcement fine-tuning for language models by improving rollout accuracy and data co-influence, leading to consistently better post-RL performance.</details> |
| 2025.0526 |  REA-RL  |  HIT  | [Paper](https://arxiv.org/abs/2505.19862)<br />[GitHub](https://github.com/hexuandeng/REA-RL) ![](https://img.shields.io/github/stars/hexuandeng/REA-RL) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Introduces REA-RL, which enhance the efficiency of LRMs by introducing a reflection model for efficient scaling online, and a reflection reward to prevent non-reflective responses.</details> |
| 2025.0527 | ConciseR  |    Tencent Hunyuan   | [Paper](http://arxiv.org/abs/2505.21178)<br />[GitHub](https://github.com/nick7nlp/ConciseR) ![](https://img.shields.io/github/stars/nick7nlp/ConciseR) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper proposes a simple yet effective two-stage reinforcement learning framework for achieving concise reasoning in LLMs, named ConciseR.</details> |
| 2025.0527 |  VeriFree  |   Sea AI Lab  | [Paper](http://arxiv.org/abs/2505.21493)<br />[GitHub](https://github.com/sail-sg/VeriFree) ![](https://img.shields.io/github/stars/sail-sg/VeriFree)  | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper proposes a verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer.</details> |
| 2025.0527 |  One-Shot-EM  |   Ubiquant   | [Paper](http://arxiv.org/abs/2505.20282)<br />[GitHub](https://github.com/zitian-gao/one-shot-em) ![](https://img.shields.io/github/stars/zitian-gao/one-shot-em) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper trained 13,440 large language models and found that entropy minimization requires only a single unlabeled data and 10 steps optimization to achieve performance improvements greater than those obtained using thousands of data and carefully designed rewards in rule-based reinforcement learning.</details> |
| 2025.0528  |  Entropy-RL |  Shanghai AI Lab & THU | [Paper](https://arxiv.org/abs/2505.22617)<br />[GitHub](https://github.com/PRIME-RL/Entropy-Mechanism-of-RL) ![](https://img.shields.io/github/stars/PRIME-RL/Entropy-Mechanism-of-RL) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper aims to overcome a major obstacle in scaling RL for reasoning with LLMs, namely the collapse of policy entropy.</details> |
| 2025.0528  |  RENT-RL |  CMU | [Paper](https://arxiv.org/abs/2505.22660)<br />[GitHub](https://github.com/satrams/rent-rl) ![](https://img.shields.io/github/stars/satrams/rent-rl) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>RENT: Reinforcement Learning via Entropy Minimization is a fully unsupervised reinforcement learning method that improves reasoning performance by using the model's own confidence as a reward. </details> |
| 2025.0528 |  SynLogic  |  MiniMax-AI  | [Paper](http://arxiv.org/abs/2505.19641)<br />[GitHub](https://github.com/MiniMax-AI/SynLogic) ![](https://img.shields.io/github/stars/MiniMax-AI/SynLogic) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper presents  SynLogic, a data synthesis framework and dataset that generates diverse logical reasoning data at scale,  encompassing 35 diverse logical reasoning tasks.</details> |
| 2025.0530 |  ProRL   |   Nvidia   | [Paper](https://arxiv.org/abs/2505.24864) | [Nemotron-Qwen-1.5B](https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B) | ‚Äî‚Äî | <details><summary>Click</summary>This paper challenges prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling.</details> |
| 2025.0530 | ReasoningGym  |    OpenThought    | [Paper]()<br />[GitHub](https://github.com/open-thought/reasoning-gym/) ![](https://img.shields.io/github/stars/open-thought/reasoning-gym) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper introduces Reasoning Gym, a library of reasoning environments for reinforcement learning with verifiable rewards. It provides over 100 data generators and verifiers spanning multiple domains including algebra, arithmetic, computation, cognition, geometry, graph theory, logic, and various common games.</details> |
| 2025.0530 | AReaL  |    InclusionAI    | [Paper](https://arxiv.org/abs/2505.24298)<br />[GitHub](https://github.com/inclusionAI/AReaL) ![](https://img.shields.io/github/stars/inclusionAI/AReaL) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>AReaL introduces a fully asynchronous reinforcement learning system for language reasoning tasks, decoupling rollout generation from model training to significantly improve GPU utilization and training speed‚Äîachieving up to 2.57√ó speedup over synchronous systems‚Äîwhile maintaining or improving model performance.</details> |
| 2025.0602 |  HighEntropyRL  |     Qwen&THU  | [Paper](https://arxiv.org/abs/2506.01939)<br />[Project](https://shenzhi-wang.github.io/high-entropy-minority-tokens-rlvr/) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>High-entropy minority tokens play an outsized role in RLVR training. This paper provides actionable insights into optimizing reward design.</details> |
| 2025.0602 |  RLVR-Decomposed  |    Princeton   | [Paper](https://arxiv.org/abs/2506.01347)<br />[GitHub](https://github.com/TianHongZXY/RLVR-Decomposed) ![](https://img.shields.io/github/stars/TianHongZXY/RLVR-Decomposed) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Shows that penalizing incorrect answers alone can significantly boost LLM reasoning via PPO‚Äîchallenging conventional RLHF approaches. </details> |
| 2025.0602 |  Writing-Zero  |   Star Writing   | [Paper](https://arxiv.org/abs/2506.00103) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Applies RLVR to creative tasks like story writing by converting non-verifiable tasks into verifiable subgoals.</details> |
| 2025.0602 |  SRPO |  ByteDance Seed & OSU  | [Paper](https://arxiv.org/abs/2506.00103) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Proposes a two-stage RL framework combining self-reflection and Group Relative Policy Optimization to boost multimodal reasoning.  </details> |
| 2025.0603 |  KDRL  |  HIT&Huawei   | [Paper](https://arxiv.org/abs/2506.02208) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Presents KDRL, a unified framework combining knowledge distillation and RL to enhance LLM reasoning post-training, improving sample efficiency and generalization.  </details> |
| 2025.0603 |  TRePO  |   Amazon   | [Paper](https://arxiv.org/abs/2506.02553) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Proposes that response-level rewards suffice for effective online RL in LLMs, offering a mathematical foundation for this approach.  </details> |
| 2025.0603 |   Critique-GRPO |  CUHK   | [Paper](https://arxiv.org/abs/2506.03106)<br />[GitHub](https://github.com/zhangxy-2019/critique-GRPO) ![](https://img.shields.io/github/stars/zhangxy-2019/critique-GRPO) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Combines natural language critiques with numerical rewards in RL to overcome performance plateaus in LLM reasoning tasks.  </details> |
| 2025.0603 |   Unlikeliness Rewards  |    CMU   | [Paper](https://www.arxiv.org/abs/2506.02355) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>The paper introduces an unlikeliness reward mechanism to address biases in Group Relative Policy Optimization (GRPO), enhancing the diversity and accuracy of large language models on structured tasks like formal theorem proving. </details> |
| 2025.0604 |  Reflect/Retry/Reward  |    Writer    | [Paper](https://arxiv.org/abs/2505.24726) | - | - | <details><summary>Click</summary>A method using self-reflection and reinforcement learning improves the performance of large language models, especially with limited feedback, by rewarding self-reflections that lead to better task performance..</details> |
| 2025.0604 |   RewardAnything  |    PKU&WeChatAI    | [Paper](https://arxiv.org/abs/2506.03637)<br />[GitHub](https://github.com/WisdomShell/RewardAnything) ![](https://img.shields.io/github/stars/WisdomShell/RewardAnything) | [RewardAnything-8B-v1](https://huggingface.co/zhuohaoyu/RewardAnything-8B-v1) | ‚Äî‚Äî | <details><summary>Click</summary>Introduces principle-following reward models that generalize across tasks by adhering to natural language specifications, improving alignment without retraining.</details> |
| 2025.0605 |    ALP  |    Stanford   | [Paper](https://arxiv.org/abs/2506.05256) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Introduces adaptive length penalties in reinforcement learning to encourage concise reasoning in large language models, enhancing efficiency without sacrificing performance.</details> |
| 2025.0605 |   PatternSelection |    HKU   | [Paper](https://arxiv.org/abs/2506.04695) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Explores mechanisms for selecting reasoning patterns in reinforcement learning for language models, aiming to enhance decision-making processes.</details> |
| 2025.0605 |   LogicPuzzleRL |      PKU   | [Paper](https://arxiv.org/abs/2506.04821)<br />[GitHub](https://github.com/wongzhenhao/GameRL)  ![](https://img.shields.io/github/stars/wongzhenhao/GameRL) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Utilizes reinforcement learning on custom logic puzzles to cultivate robust mathematical reasoning in large language models.</details> |
| 2025.0605 |  DOTS  |   UIUC&NYU    | [Paper](https://arxiv.org/abs/2506.05316) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Proposes methods to improve data efficiency in reinforcement fine-tuning of LLMs through difficulty-targeted online data selection and rollout replay.</details> |
| 2025.0605 |  ether0  |     FutureHouse   | [Paper](https://storage.googleapis.com/aviary-public/ether0_preprint.pdf)<br />[GitHub](https://github.com/Future-House/ether0)  ![](https://img.shields.io/github/stars/Future-House/ether0) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>A 24B parameter model trained for scientific reasoning in chemistry, capable of generating molecular structures from natural language prompts.</details> |
| 2025.0605 | Writing-RL | Alibaba | [Paper](https://arxiv.org/abs/2506.05760) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Curriculum-based RL improves long-form narrative coherence through structured rewards.</details> |
| 2025.0606 | Confidence | Moscow | [Paper](https://arxiv.org/abs/2506.06395) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Confidence-driven few-shot RL fine-tuning improves sample efficiency without reward supervision.</details> |
| 2025.0607 | Thinking vs. Doing | CMU | [Paper](https://arxiv.org/abs/2506.07976)<br />[GitHub](https://github.com/test-time-interaction/TTI) ![](https://img.shields.io/github/stars/test-time-interaction/TTI) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>LLMs learn test-time interaction: when to think, when to act‚Äîenhances reasoning efficiency.</details> |
| 2025.0607 | OptimalReasoning | THU | [Paper](https://arxiv.org/abs/2506.07104) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Theoretical study on RL-optimality gap for chain-of-thought reasoning.</details> |
| 2025.0608 | YouronMath | Keio Univ | [Paper](https://arxiv.org/abs/2506.08935)<br />[GitHub](https://github.com/shinandrew/YouronMath) ![](https://img.shields.io/github/stars/shinandrew/YouronMath) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Gamified interface improves LLM math performance via reward shaping and iterative gameplay.</details> |
| 2025.0608 | Play to Generalize | Rice | [Paper](https://arxiv.org/abs/2506.08011)<br />[GitHub](https://github.com/yunfeixie233/ViGaL) ![](https://img.shields.io/github/stars/yunfeixie233/ViGaL) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Trains reasoning via gameplay to transfer skills across tasks.</details> |
| 2025.0608 | RPT | Microsoft | [Paper](https://arxiv.org/abs/2506.08007) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Uses RL objectives during pretraining to equip LLMs with better downstream reasoning capabilities.</details> |
| 2025.0608 | MARL | WM Univ. | [Paper](https://arxiv.org/abs/2506.08379) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>LLMs critique each other in a reflective multi-agent framework to iteratively refine reasoning chains.</details> |
| 2025.0608 | RLT | Alibaba | [Paper](https://arxiv.org/abs/2506.08388) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>RL teachers dynamically allocate thinking-time during inference to balance latency and accuracy.</details> |
| 2025.0608 | SwS | Microsoft | [Paper](https://arxiv.org/abs/2506.08989) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>LLM self-assesses its weaknesses, then generates challenging tasks to improve via RL.</details> |
| 2025.0608 | RuleReasoner | UCLA | [Paper](https://arxiv.org/abs/2506.08672) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Blends rule-based logic with RL-driven dynamic sampling to solve structured reasoning problems.</details> |
| 2025.0608 | Bingo | Microsoft | [Paper](https://arxiv.org/abs/2506.08125) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>RL method improves reasoning by amplifying attention on critical intermediate steps.</details> |
| 2025.0609 | CoRT | Qwen | [Paper](https://arxiv.org/abs/2506.09820)<br />[GitHub](https://github.com/ChengpengLi1003/CoRT) ![](https://img.shields.io/github/stars/ChengpengLi1003/CoRT) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Tool-augmented RL trains LLMs to reason via code synthesis and self-refinement loops.</details> |
| 2025.0609 | VerIF | THU | [Paper](https://arxiv.org/abs/2506.09942)<br />[GitHub](https://github.com/THU-KEG/VerIF) ![](https://img.shields.io/github/stars/THU-KEG/VerIF) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Verification-first RL training: modularly verifies and rewrites faulty LLM outputs during policy updates.</details> |
| 2025.0609 | Router-R1 | UIUC | [Paper](https://arxiv.org/abs/2506.09033)<br />[GitHub](https://github.com/ulab-uiuc/Router-R1) ![](https://img.shields.io/github/stars/ulab-uiuc/Router-R1) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>RL-based routing policies optimize multi-round tool use and answer aggregation.</details> |
| 2025.0609 | RePO | CUHK + AILab | [Paper](https://arxiv.org/abs/2506.09340)<br />[GitHub](https://github.com/SihengLi99/RePO) ![](https://img.shields.io/github/stars/SihengLi99/RePO) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Replay-Enhanced Policy Optimization: improves sample efficiency and stability of reasoning training.</details> |
| 2025.0609 | SSA | CUNY | [Paper](https://arxiv.org/abs/2506.09014) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Promotes consistency by aligning reasoning traces across training samples with shared structure.</details> |
| 2025.0609 | ComfyUI-R1 | HIT & Alibaba | [Paper](https://arxiv.org/abs/2506.09790)<br />[GitHub](https://github.com/AIDC-AI/ComfyUI-Copilot) ![](https://img.shields.io/github/stars/AIDC-AI/ComfyUI-Copilot) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Reasoning-powered LLM agent for UI pipeline automation inspired by ComfyUI workflows.</details> |
| 2025.0609 | Learning to Clarify | Adobe | [Paper](https://arxiv.org/abs/2506.06964) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>LLMs learn when and how to ask clarification questions via reward-weighted fine-tuning.</details> |
| 2025.0610 | Magistral | Mistral AI | [Paper](https://arxiv.org/abs/2506.10910) | [Magistral-Small-2506](https://huggingface.co/mistralai/Magistral-Small-2506) | ‚Äî‚Äî | <details><summary>Click</summary>First RL-trained reasoning LLM from Europe. Strong multilingual chain-of-thought and tool use. Open-source (Apache 2.0).</details> 
| 2025.0610 | FastEasy & Deep Hard | FDU | [Paper](https://arxiv.org/abs/2506.10446) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Applies dynamic penalty on output length to focus model effort on harder inputs.</details> |
| 2025.0610 | PAG | ByteDance | [Paper](https://arxiv.org/abs/2506.10406)<br />[GitHub](https://jackory.github.io/pag/) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>LLMs generate, verify, and correct responses in multi-turn RL framework inspired by verifier-agent loops.</details> |
| 2025.0610 | SAL | MIT | [Paper](https://arxiv.org/abs/2506.10943) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Explores self-adjusting LLM behaviors at inference using RL-inspired introspection and elicitability measures.</details> |
| 2025.0610 | Unsupervised Elicitation | Anthropic | [Paper](https://arxiv.org/abs/2506.10943) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Reveals hidden reasoning capacities without supervision‚Äîimplications for reward-free training.</details> |
| 2025.0611 |  LearnAlign |   CUHK   | [Paper](https://arxiv.org/abs/2506.11480) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Gradient-alignment-driven reasoning data selection for better RL fine-tuning of LLMs.</details> |
| 2025.0611 |  Continue-Thinking Token |     CTK    | [Paper](https://arxiv.org/abs/2506.11274) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>New token inserted at inference to trigger deeper reasoning steps with zero-shot generalization.</details> |
| 2025.0611 |  TreeRL  |    THU-DM   | [Paper](https://arxiv.org/abs/2506.11902)<br />[GitHub](https://github.com/THUDM/TreeRL) ![](https://img.shields.io/github/stars/THUDM/TreeRL) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Combines on-policy RL and tree search for interpretable decision traces in reasoning tasks.</details> |
| 2025.0617 |  MiniMax-M1 |   Minimax  | [Paper](https://arxiv.org/abs/2506.13585)<br />[GitHub](https://github.com/MiniMax-AI/MiniMax-M1) | - | - | <details><summary>Click</summary>A hybrid-attention reasoning model called MiniMax-M1, featuring a Mixture-of-Experts architecture and lightning attention mechanism, is introduced for efficient long-input processing and reinforcement learning.</details> |
| 2025.0625 |  GSPO  |   QwenLM  | [Paper](https://arxiv.org/abs/2507.18071) | - | - | <details><summary>Click</summary>Group Sequence Policy Optimization (GSPO) is a reinforcement learning algorithm that improves training efficiency and performance of large language models by using sequence-level importance ratios and operations.</details> |
| 2025.0722 |  MiroMind-M1 |   MiroMind  | [Paper](https://arxiv.org/abs/2507.14683)<br />[GitHub](https://github.com/MiroMindAsia/MiroMind-M1) | - | - | <details><summary>Click</summary>The MiroMind-M1 series, built on Qwen-2.5, introduces fully open-source reasoning language models with state-of-the-art performance in mathematical reasoning through Context-Aware Multi-Stage Policy Optimization.</details> |
| 2025.0808 | DFT |   Southeast Univ.  | [Paper](https://arxiv.org/abs/2508.05629)<br />[GitHub](https://github.com/yongliang-wu/DFT) | - | - | <details><summary>Click</summary>Dynamic Fine-Tuning (DFT) improves the generalization of Large Language Models (LLMs) by dynamically rescaling gradients, outperforming standard Supervised Fine-Tuning (SFT) and showing competitive results in offline reinforcement learning.</details> |
| 2025.0808 |  R-Zero  |  Tencent AI, UW | [Paper](https://arxiv.org/abs/2508.05004)<br />[GitHub](https://github.com/Chengsong-Huang/R-Zero) | - | - | <details><summary>Click</summary>R-Zero is a self-evolving framework that autonomously generates and learns from its own training data, improving reasoning capabilities in LLMs without human-curated tasks.</details> |
| 2025.0811 |  GLM-4.5 |  Zhipu AI   | [Paper](https://arxiv.org/abs/2508.06471)<br />[GitHub](https://github.com/zai-org/GLM-4.5) | - | - | <details><summary>Click</summary>GLM-4.5, a Mixture-of-Experts large language model with 355B parameters, achieves strong performance across agentic, reasoning, and coding tasks using multi-stage training and reinforcement learning.</details> |
| 2025.0822 |   InternS1 |  Shanghai AI Lab   | [Paper](https://arxiv.org/abs/2508.15763)<br />[GitHub](https://github.com/InternLM/Intern-S1) | - | - | <details><summary>Click</summary>Intern-S1, a multimodal Mixture-of-Experts model with extensive pre-training and reinforcement learning, achieves top-tier performance in general reasoning and outperforms closed-source models in scientific tasks.</details> |
| 2025.0825 |  SvS |  UCLA&MS  | [Paper](https://arxiv.org/abs/2508.14029)<br />[GitHub](https://github.com/MasterVito/SvS) | - | - | <details><summary>Click</summary>An online self-play strategy with variational problem synthesis for RLVR training maintains policy entropy and improves Pass@k performance on reasoning benchmarks.</details> |
| 2025.0827 |  TreePO  |   ByteDance Seed   | [Paper](https://arxiv.org/abs/2508.17445)<br />[GitHub](https://github.com/multimodal-art-projection/TreePO) | - | - | <details><summary>Click</summary>TreePO, a self-guided rollout algorithm for sequence generation, reduces computational cost and enhances exploration diversity in reinforcement learning for large language models.</details> |
| 2025.0905 |  HPT  |   THU  | [Paper](https://arxiv.org/abs/2509.04419)<br />[GitHub](https://github.com/TsinghuaC3I/Unify-Post-Training) | - | - | <details><summary>Click</summary>A unified policy gradient estimator and Hybrid Post-Training algorithm effectively combine online and offline data for post-training language models, improving performance across various benchmarks.</details> |
| 2025.0909 |  TraceRL |   Princeton   | [Paper](https://arxiv.org/abs/2509.06949)<br />[GitHub](https://github.com/Gen-Verse/dLLM-RL) | - | - | <details><summary>Click</summary>TraceRL enhances diffusion language models with trajectory-aware reinforcement learning, improving reasoning performance on complex tasks and enabling flexible sampling.</details> |
| 2025.0910 |  Parallel-R1 | Tencent AI | [Paper](https://arxiv.org/abs/2509.07980)<br />[GitHub](https://github.com/zhengkid/Parallel-R1) | - | - | <details><summary>Click</summary>Parallel-R1, a reinforcement learning framework, enhances large language models' reasoning capabilities by enabling parallel thinking through a progressive curriculum, leading to significant performance improvements on math benchmarks.</details> |

### Multimodal Models
| Date      | Project               | Org                | Intro                                                        | HF Model                                                     | HF Dataset                                                   | Takeaway Messages                                                 |
| --------- | --------------------- | ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2025.0128 | Open-R1-MultiModal    | LLMs Lab           | [GitHub](https://github.com/EvolvingLMMs-Lab/open-r1-multimodal)<br />[More](#open-r1-mm) | [Qwen2-VL-2B-GRPO-8k](https://huggingface.co/lmms-lab/Qwen2-VL-2B-GRPO-8k)<br />[Qwen2-VL-7B-GRPO-8k](https://huggingface.co/lmms-lab/Qwen2-VL-7B-GRPO-8k) | [multimodal-open-r1-8k-verified](https://huggingface.co/datasets/lmms-lab/multimodal-open-r1-8k-verified) | <details><summary>Click</summary>Open-R1-MultiModal provides an open-source replication of R1-Zero-like RL for Multimodal LLMs, aiming to enhance complex visual reasoning. It demonstrates the effectiveness of these RL techniques for boosting multimodal performance and promotes reproducibility in the field.</details> |
| 2025.0202 | R1-V                  | Deep Agent         | [Blog](https://deepagent.notion.site/rlvr-in-vlms)<br />[GitHub](https://github.com/Deep-Agent/R1-V)<br />[More](#r1v) | ‚Äî‚Äî                                                           | [Clevr_CoGenT_TrainA_R1](https://huggingface.co/datasets/MMInstruction/Clevr_CoGenT_TrainA_R1) | <details><summary>Click</summary>R1-V applies RL, specifically RLV-Instruct, to fine-tune VLMs. It enhances complex visual reasoning and instruction-following capabilities in VLMs beyond standard supervised fine-tuning.</details> |
| 2025.0215 | VLM-R1                | OmAI Lab           | [Blog](https://om-ai-lab.github.io/index.html) <br />[GitHub](https://github.com/om-ai-lab/VLM-R1)<br />[More](#vlmr1) | [OVD](https://huggingface.co/omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321)<br />[Math](https://huggingface.co/omlab/VLM-R1-Qwen2.5VL-3B-Math-0305) <br />[REC](https://huggingface.co/omlab/Qwen2.5VL-3B-VLM-R1-REC-500steps) | ‚Äî‚Äî                                                           | <details><summary>Click</summary>VLM-R1 applies R1-style RL to VLMs, improving stability and generalization on visual reasoning tasks. It shows that RL enhances VLM generalization beyond standard fine-tuning, achieving SOTA results, particularly on complex or out-of-domain benchmarks.</details> |
| 2025.0303 | Visual-RFT                | SJTU & Shanghai AI Lab & CUHK        | [Paper](https://arxiv.org/pdf/2503.01785)<br />[GitHub](https://github.com/Liuziyu77/Visual-RFT)<br />[More](#research) | [Reasoning Grounding](https://huggingface.co/Zery/Qwen2-VL-7B_visual_rft_lisa_IoU_reward) | [COCO_base65](https://huggingface.co/datasets/laolao77/ViRFT_COCO_base65)<br />[COCO](https://huggingface.co/datasets/laolao77/ViRFT_COCO)<br />[COCO_8_classes_4_shot](https://huggingface.co/datasets/laolao77/ViRFT_COCO_8_cate_4_shot)<br />[LVIS_few_shot](https://huggingface.co/datasets/laolao77/ViRFT_LVIS_few_shot)<br />[Flower_4_shot](https://huggingface.co/datasets/laolao77/ViRFT_CLS_flower_4_shot)<br />[FGVC_Aircraft_4_shot](https://huggingface.co/datasets/laolao77/ViRFT_CLS_fgvc_aircraft_4_shot)<br />[Car196_4_shot](https://huggingface.co/datasets/laolao77/ViRFT_CLS_car196_4shot)<br />[Pets37_4_shot](https://huggingface.co/datasets/laolao77/ViRFT_CLS_pets37_4shot)                                                           | <details><summary>Click</summary>Visual-RFT introduces Visual Reinforcement Fine-tuning, which extends reinforcement learning with verified rewards on visual perception tasks that are effective with limited data for fine-tuning.</details> |
| 2025.0306 | R1-VLM                | GroundLight        | [Blog](https://www.groundlight.ai/blog/visual-reasoning-models)<br />[GitHub](https://github.com/groundlight/r1_vlm)<br />[More](#r1-vlm) | ‚Äî‚Äî                                                           | ‚Äî‚Äî                                                           | <details><summary>Click</summary>R1-VLM enhances VLMs using RL, contributing significantly improved performance on complex visual reasoning tasks (spatial, counting, logic) where standard models falter. It shows that RL effectively unlocks advanced, multi-step reasoning capabilities specifically for vision-language understanding.</details> |
| 2025.0310 | VisualThinker-R1-Zero | TurningPoint       | [Paper](https://arxiv.org/pdf/2503.05132) <br />[GitHub](https://github.com/turningpoint-ai/VisualThinker-R1-Zero)<br />[More](#visual-r1-zero) | [VisualThinker-R1-Zero](https://huggingface.co/turningpoint-ai/VisualThinker-R1-Zero) | ‚Äî‚Äî                                                           | <details><summary>Click</summary>VisualThinker-R1-Zero adapts the R1-Zero RL paradigm (no supervised fine-tuning) to VLMs, achieving SoTa visual reasoning. It shows that complex visual reasoning can be effectively cultivated directly via RL on a base VLM, bypassing supervised data needs.</details> |
| 2025.0310 | MM-EUREKA | USTC & ZTE & NEU      | [Paper](https://arxiv.org/pdf/2503.07365) <br />[Github](https://github.com/ModalMinds/MM-EUREKA) <br /> [More](#mm-eureka) | [MM-Eureka-Qwen-7B](https://huggingface.co/FanqingM/MM-Eureka-Qwen-7B) | [MM-Eureka-Dataset](https://huggingface.co/datasets/FanqingM/MM-Eureka-Dataset)       | <details><summary>Click</summary>MM-EUREKA reproduces key characteristics of text-based RL systems like DeepSeek-R1 in the multimodal space, which demonstrates that both instruction-tuned and pre-trained models can develop strong multimodal reasoning capabilities through rule-based RL without supervised fine-tuning, showing superior data efficiency compared to alternative approaches. </details> |
| 2025.0310 | Curr-ReFT | Shanghai AI Lab & SJTU & HKU       | [Paper](https://arxiv.org/pdf/2503.07065)<br />[GitHub](https://github.com/ding523/Curr_REFT)<br />[More](#curr-reft) | [3B-Curr-ReFT](https://huggingface.co/ZTE-AIM/3B-Curr-ReFT)<br />[7B-Curr-ReFT](https://huggingface.co/ZTE-AIM/7B-Curr-ReFT) | [Curr-ReFT-data](https://huggingface.co/datasets/ZTE-AIM/Curr-ReFT-data)       | <details><summary>Click</summary>Curr-ReFT proposes a Curriculum Reinforcement Finetuning strategy to enhance the out-of-distribution generalization and reasoning abilities. The curriculum paradim ensures steady progression. Moreover, a rejected sampling-based self-improvement is proposed to maintain the fundamental capabilities of VLMs through selective learning from high-quality multimodal and language examples. </details> |
| 2025.0311 | LLM-R1                | CUHK & Ant Group   | [Paper](https://arxiv.org/pdf/2503.07536)<br />[GitHub](https://github.com/TideDra/lmm-r1) | ‚Äî‚Äî                                                           | ‚Äî‚Äî                                                           | <details><summary>Click</summary>LLM-R1 contributes the RMAVO algorithm to stably enhance LLM reasoning using RL, preventing reward hacking and achieving SOTA results with smaller models via an open-source implementation. It shows that reward model assistance in value optimization is key for stable RL.</details> |
| 2025.0311 | Vision-R1             | ECNU & Xiaohongshu | [Paper](https://arxiv.org/abs/2503.06749)<br />[GitHub](https://github.com/Osilly/Vision-R1) | ‚Äî‚Äî                                                           | [Vision-R1-cold](https://huggingface.co/datasets/Osilly/Vision-R1-cold) | <details><summary>Click</summary>Vision-R1 adapts the R1-Zero RL paradigm for VLMs, training them on visual reasoning chains. Its contribution is significantly boosting complex multimodal reasoning performance. It shows that RL applied to explicit reasoning steps effectively enhances VLM capabilities.</details> |
| 2025.0311 | MMR1             | NTU & SUTD & LASA | [GitHub](https://github.com/LengSicong/MMR1) | [MMR1-Math-v0-7B](https://huggingface.co/MMR1/MMR1-Math-v0-7B)    | [MMR1-Math-RL-Data-v0](https://huggingface.co/datasets/MMR1/MMR1-Math-RL-Data-v0) | <details><summary>Click</summary>MMR1-Math-v0 achieves state-of-the-art performance among open-source 7B multimodal models, competing effectively even against proprietary models with significantly larger parameter sizes‚Äîall trained using only 6k carefully curated data instances.</details> |
| 2025.0315 | MetaSpatial                | Northwestern University                                | [Paper](https://arxiv.org/abs/2503.18470)<br />[Project](https://github.com/PzySeere/MetaSpatial)<br />[GitHub](https://github.com/PzySeere/MetaSpatial) | ‚Äî‚Äî                                                           | [3D_Reasoning](https://huggingface.co/datasets/zhenyupan/3d_layout_reasoning)                                                          | <details><summary>Click</summary>MetaSpatial leverages reinforcement learning to enhance 3D spatial reasoning in vision-language models (VLMs), enabling more structured, realistic, and adaptive scene generation for applications in the metaverse, AR/VR, and game development.</details> |
| 2025.0327 | Reason-RFT             | PKU & BAAI & CASIA & School of Artificial Intelligence, University of Chinese Academy of Sciences | [Paper](https://arxiv.org/pdf/2503.20752)<br />[GitHub](https://github.com/tanhuajie/Reason-RFT)<br />[Project](https://tanhuajie.github.io/ReasonRFT/) | ‚Äî‚Äî                                                           | [tanhuajie2001/Reason-RFT-CoT-Dataset](https://huggingface.co/datasets/tanhuajie2001/Reason-RFT-CoT-Dataset/) | <details><summary>Click</summary>Reason-RFT introduces a two-phase training paradim: (1) SFT with CoT data to activate reasoning potential, followed by (2) GRPO-based reinforcement learning to enhance generalization, which further has potential applications in Emobodied AI.</details> |
| 2025.0404 | MAYE           | SJTU & GAIR                | [Paper](https://arxiv.org/pdf/2504.02587)<br />[GitHub](https://github.com/GAIR-NLP/MAYE) |‚Äî‚Äî  | [ManTle/MAYE](https://huggingface.co/datasets/ManTle/MAYE) | <details><summary>Click</summary>MAYE is a transparent, reproducible framework and a comprehensive evaluation scheme for applying reinforcement learning (RL) to vision-language models (VLMs). Its codebase is developed entirely from scratch without relying on any existing RL toolkits.</details> |
| 2025.0408 | Step-R1-V-Mini           | StepFun               | [Website](https://platform.stepfun.com) |‚Äî‚Äî  | ‚Äî‚Äî | <details><summary>Click</summary>Step-R1-V-Mini excels in the domain of visual reasoning, while also demonstrating top-tier performance in mathematical, code, and textual reasoning tasks. It supports a context length of 100k.</details> |
| 2025.0409 | Kimi-VL-Thinking           | Kimi Team               | [Technical Report](https://github.com/MoonshotAI/Kimi-VL/blob/main/Kimi-VL.pdf)<br />[GitHub](https://github.com/MoonshotAI/Kimi-VL) |[moonshotai/Kimi-VL-A3B-Thinking](https://huggingface.co/moonshotai/Kimi-VL-A3B-Thinking)  | ‚Äî‚Äî | <details><summary>Click</summary>Kimi-VL-Thinking is designed to enhance long-horizon reasoning capabilities in vision-language tasks.  Built on a foundation of long CoT SFT and RL, with only 2.8 parameters,  Kimi-VL-Thinking achieves strong performance across a range of tasks requiring long-term reasoning. It excels in domains such as MMMU, MathVision, and MathVista, achieving impressive scores of 61.7, 36.8, and 71.3, respectively.</details> |
| 2025.0409 | VideoChat-R1           | Shanghai AI Lab & NJU & ZJU & USTC & Shanghai Innovation Institute & SIAT               | [Paper](https://arxiv.org/pdf/2504.06958)<br />[GitHub](https://github.com/OpenGVLab/VideoChat-R1)  | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>VideoChat-R1 provides a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, which exhibiting remarkable performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. </details> |
| 2025.0410 | Perception-R1           | HUST & BUPT & StepFun & JHU & Tsinghua University        | [Paper](https://arxiv.org/pdf/2504.07954)<br />[GitHub](https://github.com/linkangheng/PR1)  | [Perception-R1](https://huggingface.co/collections/Kangheng/perception-r1-67f6b14f89d307a0ece985af) | [Perception-R1](https://huggingface.co/collections/Kangheng/perception-r1-67f6b14f89d307a0ece985af) | <details><summary>Click</summary>Perception-R1 explores the effects of RL on different perception tasks, the researchers observe that the percep- tual perplexity is a major factor in determining the effectiveness of RL. The scalable Perception-R1 achieves remarkable performance on the perception tasks.  </details> |
| 2025.0410 | VL-Rethinker           | TIGER-Lab        | [Paper](https://arxiv.org/pdf/2505.00703)<br />[GitHub](https://github.com/CaraJ7/T2I-R1) | [TIGER-Lab/VL-Rethinker-7B](https://huggingface.co/TIGER-Lab/VL-Rethinker-7B)<br />[TIGER-Lab/VL-Rethinker-72B](https://huggingface.co/TIGER-Lab/VL-Rethinker-72B) | ‚Äî‚Äî | <details><summary>Click</summary>VL-Rethinker proposes Selective Sample Replay (SSR) and Forced Rethinking to enhance fast-thinking models.The model achieves remarkable performance on multi-disciplinary benchmarks.  </details> |
| 2025.0501 | T2I-R1           | CUHK MMLab & CUHK MiuLar Lab & Shanghai AI Lab        | [Paper](https://arxiv.org/pdf/2504.08837)<br />[GitHub](https://github.com/TIGER-AI-Lab/VL-Rethinker)  | [CaraJ/ORM-T2I-R1](https://huggingface.co/CaraJ/ORM-T2I-R1) | ‚Äî‚Äî | <details><summary>Click</summary>T2I-R1 is a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. The semantic-level CoT is utilized for high-level planning of the prompt, and the token-level CoT is designed for low-level pixel processing during patch-by-patch generation.  </details> |
| 2025.0516 | VisualPlanning           | Cambridge & UCL & Google       | [Paper](https://arxiv.org/pdf/2505.11409)<br />[GitHub](https://github.com/yix8/VisualPlanning)  | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary> VisualPlanning enables planning through purely visual representations, independent of text. In this paradigm, planning is executed via sequences of images that encode step-by- step inference in the visual domain, akin to how humans sketch or visualize future actions.  </details> |
| 2025.0521 | GRIT           | UCSC & eBay       | [Paper](https://arxiv.org/pdf/2505.15879)<br />[GitHub](https://github.com/eric-ai-lab/GRIT)<br />[Project](https://grounded-reasoning.github.io)<br />[Demo](https://b86dd615e41b242e22.gradio.live)  | [yfan1997/GRIT-20-InternVL-2B](https://huggingface.co/yfan1997/GRIT-20-InternVL-2B)<br />[yfan1997/GRIT-20-Qwen2.5-VL-3B](https://huggingface.co/yfan1997/GRIT-20-Qwen2.5-VL-3B) | [yfan1997/GRIT_data](https://huggingface.co/datasets/yfan1997/GRIT_data) | <details><summary>Click</summary> GRIT proposes grounded reasoning with images and text for training MLLMs to think with images. The models generate reasoning chains that interleave natural language and explicit bounding box coordinates. Moreover, built upon the GRPO algorithm, GRIT eliminates the need for annotated reasoning chains or explicit bounding box labels, requiring as few as 20 image-question-answer triplets to train the model.  </details> |
| 2025.0522 | GoT-R1           | HKU MMLab & CUHK MMLab & Sensetime & BUAA       | [Paper](https://arxiv.org/pdf/2505.17022)<br />[GitHub](https://github.com/gogoduan/GoT-R1)  | [gogoduan/GoT-R1-1B](https://huggingface.co/gogoduan/GoT-R1-1B)<br />[gogoduan/GoT-R1-7B](https://huggingface.co/gogoduan/GoT-R1-7B) | ‚Äî‚Äî | <details><summary>Click</summary> GoT-R1 applies reinforcement learning to enhance semantic-spatial reasoning in visual generation. To achieve this, a dual-stage multi-dimensional reward framework is proposed that leverages MLLMs to evaluate both the reasoning process and final output, enabling effective supervision across the entire generation pipeline. The reward system assesses semantic alignment, spatial accu- racy, and visual quality in a unified approach.   </details> |
| 2025.0529 |  Jigsaw-R1  |   ESAT-PSI  | [Paper](http://arxiv.org/abs/2505.23590)<br />[GitHub](https://github.com/zifuwanggg/Jigsaw-R1) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper provides a comprehensive study of rule-based visual RL using jigsaw puzzles as a structured experimental framework, revealing several key findings.</details> |
| 2025.0603 |  SynthRL  |   NUS&CUHK   | [Paper](https://arxiv.org/abs/2506.02096)<br />[GitHub](https://github.com/NUS-TRAIL/SynthRL) | [SynthRL](https://huggingface.co/collections/Jakumetsu/synthrl-6839d265136fa9ca717105c5) | ‚Äî‚Äî | <details><summary>Click</summary>Introduces SynthRL, a pipeline that synthesizes verifiable data to train vision-language models, boosting performance on visual math reasoning tasks. </details> |
| 2025.0603 |  Cell-o1  |   UIUC  | [Paper](https://arxiv.org/abs/2506.02911)<br />[GitHub](https://github.com/ncbi-nlp/cell-o1) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Presents Cell-o1, an LLM trained via RL to annotate single-cell RNA sequencing data, achieving expert-level reasoning in batch-level contexts.  </details> |
| 2025.0604 |   MiMo-VL  |     XiaomiMimo    | [Paper](https://arxiv.org/abs/2506.03569)<br />[GitHub](https://github.com/XiaomiMiMo/MiMo-VL) | [MiMo-VL-7B](https://huggingface.co/collections/XiaomiMiMo/mimo-vl-68382ccacc7c2875500cd212) | ‚Äî‚Äî | <details><summary>Click</summary>Details MiMo-VL-7B models achieving state-of-the-art performance in visual understanding and multimodal reasoning through mixed on-policy RL.</details> |
| 2025.0604 |  ReVisual-R1  |   ZJU&FDU   | [Paper](https://arxiv.org/abs/2506.04207)<br />[GitHub](https://github.com/CSfufu/Revisual-R1) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Introduces ReVisual-R1, a staged RL approach enhancing MLLM reasoning by combining optimized cold starts with text-only RL fine-tuning.</details> |
| 2025.0604 |   LaF-GRPO  |     PolyU    | [Paper](https://arxiv.org/abs/2506.04070)<br />[GitHub](https://github.com/YiyiyiZhao/NIG4VI) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Develops an LLM-as-Follower reward mechanism to generate in-situ navigation instructions for the visually impaired, enhancing instruction usability.</details> |
| 2025.0611 |   Visual PTRL  |   UC Berkeley   | [Paper](https://arxiv.org/abs/2506.11967) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Trains visual backbones on raw image data with reinforcement rewards‚Äîunsupervised and scalable.</details> |
| 2025.0702 |   GLM-4.1V-Thinking  |  THU   | [Paper](https://arxiv.org/abs/2507.01006)<br />[GitHub](https://github.com/THUDM/GLM-4.1V-Thinking) | - | - | <details><summary>Click</summary>A vision-language model (VLM) named GLM-4.1V-Thinking, developed with a reasoning-centric training framework, achieves state-of-the-art performance across various tasks, including STEM problem solving, video understanding, and long document understanding, outperforming larger models on many benchmarks.</details> |
| 2025.0102 | Thyme: Think Beyond Images           |  Kwai Keye Team     | [Paper](https://arxiv.org/abs/2508.11630)<br />[GitHub](https://github.com/yfzhang114/Thyme) ![](https://img.shields.io/github/stars/yfzhang114/Thyme)<br /> [More](#primerl) | [Kwai-Keye/Thyme-RL](https://huggingface.co/Kwai-Keye/Thyme-RL) <br />[Kwai-Keye/Thyme-SFT](https://huggingface.co/Kwai-Keye/Thyme-SFT) | [Training-Data](https://huggingface.co/datasets/Kwai-Keye/Thyme-RL) | <details><summary>Click</summary>Thyme transcends traditional ``thinking with images'' paradigms by autonomously generating and executing diverse image processing and computational operations through executable code, significantly enhancing performance on high-resolution perception and complex reasoning tasks. </details> |
| 2025.0907 |   Keye-VL-1.5  |     Kwai Keye Team    | [Paper](https://arxiv.org/abs/2509.01563)<br />[GitHub](https://github.com/Kwai-Keye/Keye)![](https://img.shields.io/github/stars/Kwai-Keye/Keye) | [Keye-VL-1_5-8B](https://huggingface.co/Kwai-Keye/Keye-VL-1_5-8B) | ‚Äî‚Äî | <details><summary>Click</summary>Keye-VL-1.5 is a cutting-edge Multimodal Large Language Model (MLLM) that addresses fundamental challenges in video comprehension. It features a novel Slow-Fast video encoding strategy, a progressive four-stage pre-training methodology to extend context length up to 128K tokens, and a comprehensive post-training pipeline focusing on reasoning enhancement and human preference alignment. The model demonstrates significant improvements in video understanding tasks and maintains competitive performance on general multimodal benchmarks.</details> |
| 2025.0909 |  REVPT  |  HUST  | [Paper](https://arxiv.org/abs/2509.01656)<br />[GitHub](https://github.com/ls-kelvin/REVPT) | - | - | <details><summary>Click</summary>ReVPT enhances multi-modal LLMs' visual reasoning capabilities using reinforcement learning, achieving state-of-the-art performance on visual benchmarks.</details> |
| 2025.0910 |  Mini-o3 |   ByteDance     | [Paper](https://arxiv.org/abs/2509.07969)<br />[GitHub](https://github.com/Mini-o3/Mini-o3) | - | - | <details><summary>Click</summary>Mini-o3, a system for deep, multi-turn reasoning in visual search tasks, uses an iterative data collection pipeline and over-turn masking strategy to achieve state-of-the-art performance with rich reasoning patterns.</details> |


### Agentic Applications
| Date      | Project               | Org                | Intro                                                        | HF Model                                                     | HF Dataset                                                   | Takeaway Messages                                                 |
| --------- | --------------------- | ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2025.0126 | RAGEN              | RAGEN-AI                           | [Paper](https://arxiv.org/abs/2504.20073) <br /> [GitHub](https://github.com/RAGEN-AI/RAGEN) ![](https://img.shields.io/github/stars/RAGEN-AI/RAGEN)                 | ‚Äî‚Äî                                                           | ‚Äî‚Äî                                                           | <details><summary>Click</summary>RAGEN introduces a RL framework to train reasoning-capable LLM agents for interactive, stochastic environments. Its core contribution is the Reasoning-Interaction Chain Optimization (RICO) algorithm, which jointly optimizes reasoning and action strategies by reinforcing entire trajectories.</details> |
| 2025.0203 | Verifiers          | Independent                        | [GitHub](https://github.com/willccbb/verifiers) ![](https://img.shields.io/github/stars/willccbb/verifiers) | ‚Äî‚Äî                                                           | ‚Äî‚Äî                                                           | <details><summary>Click</summary>This repository contains a set of tools for reinforcement learning with LLMs in verifiable environments. It can be used for LLM Agent RL in verifable environments.</details> |
| 2025.0207 | AgenticReasoning  |   Univ. of Oxford  | [Paper](https://arxiv.org/abs/2502.04644)<br />[GitHub](https://github.com/theworldofagents/Agentic-Reasoning) ![](https://img.shields.io/github/stars/theworldofagents/Agentic-Reasoning) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This framework introduces the Mind Map agent, which constructs a structured knowledge graph to track logical relationships, improving deductive reasoning.</details> |
| 2025.0303 | ReSearch              | Agent-RL           | [GitHub](https://github.com/Agent-RL/ReSearch) ![](https://img.shields.io/github/stars/Agent-RL/ReSearch)<br />[More](#research) | ‚Äî‚Äî                                                           | ‚Äî‚Äî                                                           | <details><summary>Click</summary>The project train LLMs from scratch, utilizing RL with GRPO to learn to reason via search operations, without reliance on pre-existing reasoning frameworks or supervised data.</details> |
| 2025.0312 | Search-R1             | UIUC & UMass Amherst | [Paper](https://arxiv.org/abs/2503.09516)<br />[GitHub](https://github.com/PeterGriffinJin/Search-R1) ![](https://img.shields.io/github/stars/PeterGriffinJin/Search-R1)<br />[More](#search-r1) | [Search-R1](https://huggingface.co/collections/PeterJinGo/search-r1-67d1a021202731cb065740f5)   | [2018 Wikipedia](https://huggingface.co/datasets/PeterJinGo/wiki-18-corpus) | <details><summary>Click</summary>The paper introduces Search-R1, a novel RL framework that enables LLMs to interact with search engines in an interleaved manner with their own reasoning. The framework is shown to be effective, with experiments demonstrating average relative improvements of 41% and 20% over RAG baselines, and providing insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning.</details> |
| 2025.0318 | R1-Searcher           | RUC                | [Paper](https://arxiv.org/pdf/2503.05592)<br />[GitHub](https://github.com/RUCAIBox/R1-Searcher) ![](https://img.shields.io/github/stars/RUCAIBox/R1-Searcher) | [Llama-3.1-8B-instruct-RAG-RL](https://huggingface.co/XXsongLALA/Llama-3.1-8B-instruct-RAG-RL) <br />[Qwen-2.5-7B-base-RAG-RL](https://huggingface.co/XXsongLALA/Qwen-2.5-7B-base-RAG-RL) | [RAG-RL-Hotpotqa](https://huggingface.co/datasets/XXsongLALA/RAG-RL-Hotpotqa-with-2wiki) | <details><summary>Click</summary>R1-Searcher enhances LLM reasoning via RL by training the model to perform adaptive model-based search during generation. This integration enables flexible thinking depth, improving reasoning efficiency and performance compared to fixed-step methods like R1-Zero.</details> |
| 2025.0319 | SWEET-RL           | Meta AI                            | [Paper](https://arxiv.org/abs/2503.15478)<br />[GitHub](https://github.com/facebookresearch/sweet_rl/) ![](https://img.shields.io/github/stars/facebookresearch/sweet_rl) | ‚Äî‚Äî                                                           | [collaborative_agent_bench](https://huggingface.co/datasets/facebook/collaborative_agent_bench) | <details><summary>Click</summary>Sweet-RL introduces a novel RL algorithm for multi-turn collaborative reasoning LLM agents. Its core contribution is improving credit assignment across long interactions by using an asymmetric actor-critic structure where the critic leverages additional training-time information for step-wise evaluation.</details> |
| 2025.0327 |  UI-R1   |   Vivo AI Lab & CUHK           | [Paper](https://arxiv.org/abs/2503.21620)<br />[GitHub](https://github.com/lll6gg/UI-R1) ![](https://img.shields.io/github/stars/lll6gg/UI-R1) | [Qwen2.5-VL-3B-UI-R1](https://huggingface.co/LZXzju/Qwen2.5-VL-3B-UI-R1) | [UI-R1-3B-Train](https://huggingface.co/datasets/LZXzju/UI-R1-3B-Train) | <details><summary>Click</summary>This paper proposes UI-R1, the first framework to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for GUI action prediction tasks.</details> |
| 2025.0404 |  DeepResearcher  |    SJTU    | [Paper](https://arxiv.org/abs/2504.03160)<br />[GitHub](https://github.com/GAIR-NLP/DeepResearcher) ![](https://img.shields.io/github/stars/GAIR-NLP/DeepResearcher) | [DeepResearcher-7b](https://huggingface.co/GAIR/DeepResearcher-7b) | ‚Äî‚Äî | <details><summary>Click</summary>This paper introduces DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions.</details> |
| 2025.0414 |  ART    |    OpenPipe   | [Blog](https://openpipe.ai/blog/art-trainer-a-new-rl-trainer-for-agents)<br />[GitHub](https://github.com/OpenPipe/ART) ![](https://img.shields.io/github/stars/OpenPipe/ART) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This release is an early alpha focused on best-in-class training efficiency and agentic multi-turn support.</details> |
| 2025.0414 | GUI-R1     |   CAS & NUS      | [Paper](https://arxiv.org/abs/2504.10458)<br />[GitHub](https://github.com/ritzz-ai/GUI-R1) ![](https://img.shields.io/github/stars/ritzz-ai/GUI-R1) | ‚Äî‚Äî | [GUI-R1](https://huggingface.co/datasets/ritzzai/GUI-R1) | <details><summary>Click</summary>This paper proposes GUI-R1, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling.</details> |
| 2025.0415 | ReTool          | ByteDance                            | [Paper](https://arxiv.org/abs/2504.11536)<br />[GitHub](https://github.com/ReTool-RL/ReTool) ![](https://img.shields.io/github/stars/ReTool-RL/ReTool)<br />[More](#retool) | [ReTool-Qwen-32B](https://huggingface.co/JoeYing/ReTool-Qwen-32B)                                                        | [ReTool-SFT](https://huggingface.co/datasets/JoeYing/ReTool-SFT)  | <details><summary>Click</summary>ReTool is a reinforcement learning framework that integrates code interpreter execution into the reasoning loop of large language models (LLMs) to improve their mathematical reasoning capabilities. The framework consists of two primary stages: cold-start supervised fine-tuning and reinforcement learning with interleaved code execution rollout, allowing the model to learn when and how to invoke tools based on outcome feedback.</details> |
| 2025.0428 |   ARTIST    |    Microsoft    | [Paper](https://www.arxiv.org/abs/2505.01441) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>ARTIST enables models to autonomously decide when, how, and which tools to invoke within multi-turn reasoning chains, leveraging outcome-based RL to learn robust strategies for tool use and environment interaction without requiring step-level supervision.</details> |
| 2025.0430 | WebThinker           | RUC                            | [Paper](https://arxiv.org/abs/2504.21776)<br />[GitHub](https://github.com/RUC-NLPIR/WebThinker) ![](https://img.shields.io/github/stars/RUC-NLPIR/WebThinker)<br />[More](#webthinker) | [WebThinker-QwQ-32B](https://huggingface.co/lixiaoxi45/WebThinker-QwQ-32B) <br />[WebThinker-R1-7B](https://huggingface.co/lixiaoxi45/WebThinker-R1-7B)<br/>[WebThinker-R1-14B](https://huggingface.co/lixiaoxi45/WebThinker-R1-14B)<br />[WebThinker-R1-32B](https://huggingface.co/lixiaoxi45/WebThinker-R1-32B)                                                           | ‚Äî‚Äî  | <details><summary>Click</summary>WebThinker is a deep research agent that empowers large reasoning models (LRMs) to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. It integrates a Deep Web Explorer module and employs an Autonomous Think-Search-and-Draft strategy, allowing for real-time report writing and information gathering.</details> |
| 2025.0506 |  SkyRL-v0  |   NovaSky-AI   | [blog](https://novasky-ai.notion.site/skyrl-v0)<br />[GitHub](https://github.com/NovaSky-AI/SkyRL) ![](https://img.shields.io/github/stars/NovaSky-AI/SkyRL) | [SkyRL-Agent-7B-v0](https://huggingface.co/NovaSky-AI/SkyRL-Agent-7B-v0) <br /> [SkyRL-Agent-8B-v0](https://huggingface.co/NovaSky-AI/SkyRL-Agent-8B-v0) <br /> [SkyRL-Agent-14B-v0](https://huggingface.co/NovaSky-AI/SkyRL-Agent-14B-v0) | [SkyRL-v0-293-data](https://huggingface.co/datasets/NovaSky-AI/SkyRL-v0-293-data) | <details><summary>Click</summary>This paper introduces SkyRL, the RL training pipeline for multi-turn tool use LLMs, optimized for long-horizon, real-environment tasks like SWE-Bench, built on top of [VeRL](https://github.com/volcengine/verl) and [OpenHands](https://github.com/All-Hands-AI/OpenHands). Using SkyRL, we are able to achieve promising results on [SWE-Bench-Verified](https://github.com/swe-bench/SWE-bench) across model lines, using around 300 samples of training data!</details> |
| 2025.0512 |  Tool-N1  |      NVIDIA      | [Paper](https://arxiv.org/abs/2505.00024)<br />[GitHub](https://github.com/NVlabs/Tool-N1) ![](https://img.shields.io/github/stars/NVlabs/Tool-N1) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper presents Nemotron-Research-Tool-N1, a family of tool-using reasoning language models. These models are trained with an R1-style reinforcement learning algorithm that uses a binary reward to supervise only the structural format and functional correctness of tool calls, without requiring explicit reasoning annotations.</details> |
| 2025.0512 |  ZeroTIR  |     FDU & Xiaohongshu     | [Paper](https://arxiv.org/abs/2505.07773)<br />[GitHub](https://github.com/yyht/openrlhf_async_pipline) ![](https://img.shields.io/github/stars/yyht/openrlhf_async_pipline) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper investigates RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples.</details> |
| 2025.0513 |  AgentCPM-GUI     |      OpenBMB     | [GitHub](https://github.com/OpenBMB/AgentCPM-GUI) ![](https://img.shields.io/github/stars/OpenBMB/AgentCPM-GUI) | [openbmb/AgentCPM-GUI](https://huggingface.co/openbmb/AgentCPM-GUI) | ‚Äî‚Äî | <details><summary>Click</summary>AgentCPM-GUI is an open-source on-device LLM agent model jointly developed by THUNLP, Renmin University of China and ModelBest. Built on MiniCPM-V with 8 billion parameters, it accepts smartphone screenshots as input and autonomously executes user-specified tasks.</details> |
| 2025.0514 |  AlphaEvolve   |    Google DeepMind   | [Blog](https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>AlphaEvolve is an evolutionary coding agent powered by large language models for general-purpose algorithm discovery and optimization.</details> |
| 2025.0515 |   GiGPO   |     NTU&Skywork        | [Paper](https://arxiv.org/abs/2505.10978)<br />[GitHub](https://github.com/langfengQ/verl-agent) ![](https://img.shields.io/github/stars/langfengQ/verl-agent) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper proposes Group-in-Group Policy Optimization (GiGPO), a novel RL algorithm that achieves fine-grained credit assignment for LLM agents while preserving the appealing properties of group-based RL: critic-free, low memory, and stable convergence. </details> |
| 2025.0516 |  AutoRefine  |  USTC  | [Paper](http://arxiv.org/abs/2505.11277)<br />[GitHub](https://github.com/syr-cn/AutoRefine) ![](https://img.shields.io/github/stars/syr-cn/AutoRefine) | [hf models]() | [hf datasets]() | <details><summary>Click</summary>This paper proposes AutoRefine, a reinforcement learning posttraining framework that adopts a new "search-and-refine-during-think" paradigm.</details> |
| 2025.0520 |   Time-R1 |      UIUC    | [Paper](https://arxiv.org/abs/2505.13508)<br />[GitHub](https://github.com/ulab-uiuc/Time-R1) ![](https://img.shields.io/github/stars/ulab-uiuc/Time-R1) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper introduces Time-R1, the first framework to endow a moderate-sized (3B-parameter) LLM with comprehensive temporal abilities: understanding, prediction, and creative generation.</details> |
| 2025.0521 |   Empirical Study |   UIUC     | [Paper](https://arxiv.org/abs/2505.15117)<br />[GitHub](https://github.com/PeterGriffinJin/Search-R1) ![](https://img.shields.io/github/stars/PeterGriffinJin/Search-R1) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper highlights several key findings: format rewards are effective in improving final performance, whereas intermediate retrieval rewards have limited impact; the scale and initialization of the LLM (general-purpose vs. reasoningspecialized) significantly influence RL outcomes; and the choice of search engine plays a critical role in shaping RL training dynamics and the robustness of the trained agent during inference.</details> |
| 2025.0521 |  StepSearch  |    SenseTime   | [Paper](https://arxiv.org/abs/2505.15107)<br />[GitHub](https://github.com/Zillwang/StepSearch) ![](https://img.shields.io/github/stars/Zillwang/StepSearch) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper introduces StepSearch, a framework for search LLMs that trained with step-wise proximal policy optimization method.</details> |
| 2025.0521 |   GUI-G1  |       RUC    | [Paper](https://arxiv.org/pdf/2505.15810)<br />[GitHub](https://github.com/Yuqi-Zhou/GUI-G1) ![](https://img.shields.io/github/stars/Yuqi-Zhou/GUI-G1) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper identifies three distinct challenges in the R1-Zero-Like training pipeline of R1-style GUI agents: grounding is harmed by longer reasoning due to grounding‚Äôs reliance on image tokens; common reward functions induce sizesensitive reward hacking; and GRPO biases agents toward simpler examples due to its objective.</details> |
| 2025.0522 |  Tool-Star |  RUC   | [Paper](https://arxiv.org/abs/2505.16410v1)<br />[GitHub](https://github.com/dongguanting/Tool-Star) ![](https://img.shields.io/github/stars/dongguanting/Tool-Star) | [Tool-Star-Qwen-3B](https://huggingface.co/dongguanting/Tool-Star-Qwen-3B) | [Multi-Tool-RL-10K](https://huggingface.co/datasets/dongguanting/Multi-Tool-RL-10K) </br> [Tool-Star-SFT-54K](https://huggingface.co/datasets/dongguanting/Tool-Star-SFT-54K) | <details><summary>Click</summary>This paper introduces Tool-Star, an RL-based framework designed to empower LLMs to autonomously invoke multiple external tools during stepwise reasoning.</details> |
| 2025.0522 |  R1-Searcher++  |  RUC   | [Paper](http://arxiv.org/abs/2505.17005)<br />[GitHub](https://github.com/RUCAIBox/R1-Searcher-plus) ![](https://img.shields.io/github/stars/RUCAIBox/R1-Searcher-plus) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>insights and contributions about RL for reasoning within 30 words.</details> |
| 2025.0522 |  ARPO  |  CUHK   | [Paper](http://arxiv.org/abs/2505.16282)<br />[GitHub](https://github.com/dvlab-research/ARPO) ![](https://img.shields.io/github/stars/dvlab-research/ARPO) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper investigates end-to-end policy optimization for vision-language-based GUI agents with the aim of improving performance on complex, long-horizon computer tasks.</details> |
| 2025.0522 |   AgentThink  |   THU&McGill  | [Paper](http://arxiv.org/abs/2505.15298) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper introduces AgentThink, a pioneering unified framework that, for the first time, integrates Chainof-Thought (CoT) reasoning with dynamic, agent-style tool invocation for autonomous driving tasks.</details> |
| 2025.0523 |  Agent-Distillation  |  KAIST  | [Paper](http://arxiv.org/abs/2505.17612)<br />[GitHub](https://github.com/Nardien/agent-distillation) ![](https://img.shields.io/github/stars/Nardien/agent-distillation) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper proposes Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools.</details> |
| 2025.0526 |  DeepEyes |   Xiaohongshu  | [Paper](http://arxiv.org/abs/2505.14362)<br />[GitHub](https://github.com/Visual-Agent/DeepEyes) ![](https://img.shields.io/github/stars/Visual-Agent/DeepEyes) | [DeepEyes-7B](https://huggingface.co/ChenShawn/DeepEyes-7B) | [DeepEyes-Datasets-47k](https://huggingface.co/datasets/ChenShawn/DeepEyes-Datasets-47k) | <details><summary>Click</summary>This paper explores the interleaved multimodal reasoning paradigm and introduce DeepEyes, a model with "thinking with images" capabilities incentivized through end-to-end reinforcement learning without the need for cold-start SFT.</details> |
| 2025.0527 |  rStar |   MRA   | [Paper](http://arxiv.org/abs/2505.21297)<br />[GitHub](https://github.com/microsoft/rStar) ![](https://img.shields.io/github/stars/microsoft/rStar) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper introduces rStar-Coder, which significantly improves LLM code reasoning capabilities by constructing a large-scale, verified dataset of 418K competition-level code problems, 580K long-reasoning solutions along with rich test cases of varying difficulty.</details> |
| 2025.0527 |  SPA-RL-Agent  |  PolyU | [Paper](https://arxiv.org/abs/2505.20732)<br />[GitHub](https://github.com/WangHanLinHenry/SPA-RL-Agent) ![](https://img.shields.io/github/stars/WangHanLinHenry/SPA-RL-Agent) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper proposes Stepwise Progress Attribution (SPA), a general reward redistribution framework that decomposes the final reward into stepwise contributions, each reflecting its incremental progress toward overall task completion.</details> |
| 2025.0528 |  WebDancer  |    Tongyi Lab    | [Paper](https://arxiv.org/abs/2505.22648)<br />[GitHub](https://github.com/Alibaba-NLP/WebAgent) ![](https://img.shields.io/github/stars/Alibaba-NLP/WebAgent) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>The paper introduces a unified, data-centric training paradigm for developing agentic web research agents, exemplified by WebDancer, which combines supervised learning and reinforcement learning to achieve strong multi-step information-seeking performance on GAIA and WebWalkerQA benchmarks.</details> |
| 2025.0529 |  ML-Agent  |  SJTU  | [Paper](http://arxiv.org/abs/2505.23723) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>This paper explores the paradigm of learning-based agentic ML, where an LLM agent learns through interactive experimentation on ML tasks using online reinforcement learning (RL).</details> |
| 2025.0530 |  Pangu DeepDiver  |   Huawei   | [Paper](https://arxiv.org/abs/2505.24332) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>The paper introduces Pangu DeepDiver, a reinforcement learning framework that equips large language models with adaptive search intensity scaling (SIS) for open-web question answering, using a new WebPuzzle dataset to improve evidence-seeking behavior under real-world ambiguity and noise.</details> |
| 2025.0601 |  VerlTool   |   TIGER AI Lab     | [GitHub](https://github.com/TIGER-AI-Lab/verl-tool) ![](https://img.shields.io/github/stars/TIGER-AI-Lab/verl-tool) | [Qwen2.5-Math-VerlTool](https://huggingface.co/VerlTool) | ‚Äî‚Äî | <details><summary>Click</summary>VerlTool is a unified and easy-to-extend tool agent training framework based on verl</details> |
| 2025.0602 | SCA  |    UCB & Meta  | [Paper](https://arxiv.org/abs/2506.01716) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>LLMs generate and solve their own tasks via a "Code-as-Task" setup, using RL for learning. Yields >2√ó gains on tool-use benchmarks.  </details> |
| 2025.0602 | MMedAgent-RL |   UNC   | [Paper](https://arxiv.org/abs/2506.00555) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Multi-agent reinforcement learning for medical reasoning with multimodal data. Promotes coordination and robustness across specialized agents. </details> |
| 2025.0603 |  CURE |   ByteDance Seed  | [Paper](https://arxiv.org/abs/2506.03136)<br />[GitHub](https://github.com/Gen-Verse/CURE) ![](https://img.shields.io/github/stars/Gen-Verse/CURE) | [reasonflux-coder](https://huggingface.co/collections/Gen-Verse/reasonflux-coder-6833109ed9300c62deb32c6b) | ‚Äî‚Äî | <details><summary>Click</summary>Introduces CURE, a framework where code generation and unit testing co-evolve through RL, enhancing code accuracy without ground-truth supervision.  </details> |
|2025.0604 |  Seed-Coder  |   ByteDance Seed   | [Paper](https://arxiv.org/abs/2506.03524)<br />[GitHub](https://github.com/ByteDance-Seed/Seed-Coder) ![](https://img.shields.io/github/stars/ByteDance-Seed/Seed-Coder) | [Seed-Coder](https://huggingface.co/collections/ByteDance-Seed/seed-coder-680de32c15ead6555c75b0e4) | ‚Äî‚Äî | <details><summary>Click</summary>Proposes a self-curating code model that generates and selects its own training data, enhancing code generation capabilities without external supervision.</details> |
| 2025.0604 |  DyMo  |     Cohere    | [Paper](https://arxiv.org/abs/2506.02918) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Presents a self-verification sampling method for LLMs to enhance tool use by predicting and verifying intermediate steps before proceeding.</details> |
| 2025.0604 |   R-Search  |     CAS    | [Paper](https://arxiv.org/abs/2506.04185)<br />[GitHub](https://github.com/QingFei1/R-Search) ![](https://img.shields.io/github/stars/QingFei1/R-Search) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Presents a multi-reward RL framework enabling LLMs to integrate reasoning with search, improving performance on complex logic and knowledge tasks.</details> |
| 2025.0605 | MedAgentGym |     Emory Univ.     | [Paper](https://arxiv.org/abs/2506.04405)<br />[GitHub](https://github.com/wshi83/MedAgentGym)  ![](https://img.shields.io/github/stars/wshi83/MedAgentGym) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Introduces a training environment for LLM agents focused on code-based medical reasoning, facilitating the development of AI in healthcare applications.</details> |
| 2025.0605 |  CI-RL |   Purdue&Microsoft    | [Paper](https://arxiv.org/abs/2506.04245) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Applies reinforcement learning to enhance contextual integrity in LLMs, aligning their outputs with privacy and safety norms.</details> |
| 2025.0611 | Grounding-R1 | Salesforce | [Blog](https://huggingface.co/blog/HelloKKMe/grounding-r1) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>GUI grounding via GRPO RL‚Äîclicks relevant areas without bounding-box or rationale supervision.</details> |
| 2025.0611 |  Agent-RLVR  |  Scale AI   | [Paper](https://arxiv.org/abs/2506.11425) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Trains software agents using both environmental feedback and expert guidance‚Äîtargeting real-world SE tasks.</details> |
| 2025.0611 |   ReVeal   |   MAR & THU    | [Paper](https://arxiv.org/abs/2506.11442) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Self-evolving agents improve code generation via iterative RL-based generate‚Äìverify cycles.</details> |
| 2025.0611 |   CAGSR-vLLM-MTC  |     UC Berkeley     | [Paper](https://arxiv.org/abs/2506.11108) | ‚Äî‚Äî | ‚Äî‚Äî | <details><summary>Click</summary>Enhances multi-turn reasoning via vLLM + self-supervised fine-tuning + RL on CoT traces.</details> |
| 2025.0704 |  WebSailor  |   Tongyi  | [Paper](https://arxiv.org/abs/2507.02592)<br />[GitHub](https://github.com/Alibaba-NLP/WebAgent/) | - | - | <details><summary>Click</summary>WebSailor, a post-training methodology, enhances open-source LLMs with sophisticated reasoning to match proprietary systems in complex information-seeking tasks.</details> |
| 2025.0729 |  ARPO |    Kuaishou   | [Paper](https://arxiv.org/abs/2507.19849)<br />[GitHub](https://github.com/dongguanting/ARPO) | - | - | <details><summary>Click</summary>Agentic Reinforced Policy Optimization (ARPO) enhances multi-turn reasoning in large language models by balancing long-horizon capabilities and tool interactions, using entropy-based adaptive rollouts and advantage attribution.</details> |
|2025.0804 |   CognitiveKernel-Pro | Tencent  | [Paper](https://arxiv.org/abs/2508.00414)<br />[GitHub](https://github.com/Tencent/CognitiveKernel-Pro) | - | - | <details><summary>Click</summary>Cognitive Kernel-Pro is an open-source multi-module agent framework that enhances AI agent robustness and performance through data curation and novel test-time strategies, achieving state-of-the-art results.</details> |
| 2025.0807 |  Agent-Lightning  |  MS  | [Paper](https://arxiv.org/abs/2508.03680)<br />[GitHub](https://github.com/microsoft/agent-lightning) | - | - | <details><summary>Click</summary>Agent Lightning is a flexible RL framework for training LLMs in various agents, using a hierarchical RL algorithm and decoupling execution from training to handle complex interactions.</details> |
| 2025.0818 |  SSRL  |    THU   | [Paper](https://arxiv.org/abs/2508.10874)<br />[GitHub](https://github.com/TsinghuaC3I/SSRL) | - | - | <details><summary>Click</summary>LLMs can serve as efficient simulators for RL tasks by leveraging internal knowledge, reducing reliance on external search engines and improving sim-to-real transfer.</details> |
| 2025.0825 |    AgentFly  | UCL   | [Paper](https://arxiv.org/abs/2508.16153)<br />[GitHub](https://github.com/Agent-on-the-Fly/AgentFly) | - | - | <details><summary>Click</summary>A novel memory-augmented reinforcement learning paradigm enables adaptive LLM agents to continually learn without fine-tuning, using episodic memory and a neural case-selection policy.</details> |
| 2025.0829 |  rStar2-Agent |  MS    | [Paper](https://arxiv.org/abs/2508.20722)<br />[GitHub](https://github.com/microsoft/rStar) | - | - | <details><summary>Click</summary>rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning, achieves state-of-the-art performance by efficiently handling complex problem-solving with advanced cognitive behaviors and minimal computational resources.</details> |
| 2025.0903 | UT-TARS-2  |  Bytedance | [Paper](https://arxiv.org/abs/2509.02544)<br />[GitHub](https://seed-tars.com/showcase/ui-tars-2/) | - | - | <details><summary>Click</summary>UI-TARS-2, a native GUI-centered agent model, addresses challenges in data scalability, multi-turn reinforcement learning, and environment stability, achieving significant improvements over its predecessor and strong baselines across various benchmarks.</details> |
| 2025.0903 |  SimpleTIR |    Tiktok  | [Paper](https://arxiv.org/abs/2509.02479)<br />[GitHub](https://github.com/ltzheng/SimpleTIR) | - | - | <details><summary>Click</summary>SimpleTIR stabilizes multi-turn Tool-Integrated Reasoning training by filtering out void turns, achieving state-of-the-art performance on math reasoning benchmarks.</details> |
| 2025.0909 |  WebExplorer |   HKUST    | [Paper](https://arxiv.org/abs/2509.06501) |- | - | <details><summary>Click</summary>WebExplorer, a data-driven approach for developing advanced web agents, achieves state-of-the-art performance in information-seeking tasks through systematic data generation and reinforcement learning.</details> |


## üåü Acknowledgment

This survey is extended and refined from the original **Awesome RL Reasoning Recipes** repo. We are deeply grateful to all contributors for their efforts, and we sincerely thank for their all interest in **Awesome RL Reasoning Recipes**. The contents of the previous repository are available [here](https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs/releases/tag/TripleR).


## üéà Citation

If you find this survey helpful, please cite our work:

```bibtex
@article{zhang2025survey,
  title={A Survey of Reinforcement Learning for Large Reasoning Models},
  author={Zhang, Kaiyan and Zuo, Yuxin and He, Bingxiang and Sun, Youbang and Liu, Runze and Jiang, Che and Fan, Yuchen and Tian, Kai and Jia, Guoli and Li, Pengfei and others},
  journal={arXiv preprint arXiv:2509.08827},
  year={2025}
}
```

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=TsinghuaC3I/Awesome-RL-for-LRMs&type=Date)](https://www.star-history.com/#TsinghuaC3I/Awesome-RL-for-LRMs&Date)
